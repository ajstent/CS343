{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Bag Of Tricks for Preventing Overfitting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    "1. What are:\n",
    " * underfitting\n",
    " * overfitting\n",
    "2. How can you detect them?\n",
    "\n",
    "Some important notes from the reading for today, plus one of my own:\n",
    "* Many deep learning models are *overparametrized*.\n",
    "* This means that they \"are typically expressive enough to perfectly fit every training example, even in datasets consisting of millions\" of data points.\n",
    "* \"Even stranger ... we can actually reduce the generalization error further by making the model even more expressive, e.g., adding layers, nodes, or training for a larger number of epochs.\" \n",
    "* \"The pattern relating the generalization gap to the complexity of the model ... can be non-monotonic, with greater complexity hurting at first but subsequently helping\". \n",
    "* \"The guarantees provided by classical learning theory ... appear powerless to explain why it is that deep neural networks generalize in the first place.\" "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trick 1: Regularization\n",
    "\n",
    "Let me show you what Bing beta said about L1 and L2 regularization!\n",
    "\n",
    "Okay, let's define L1 and L2 regularization:\n",
    "* L1: $\\frac{1}{N} \\sum_{i=1}^N (y-\\hat{y})^2 + \\lambda \\sum_{i=1}^N |w_i|$\n",
    "* L2: $\\frac{1}{N} \\sum_{i=1}^N (y-\\hat{y})^2 + \\lambda \\sum_{i=1}^N w_i^2$\n",
    "\n",
    "where $\\lambda$ is a weight for the strength of the regularization.\n",
    "\n",
    "Notes:\n",
    "* L1 regularization \"zeros out\" less significant features, so it performs feature selection\n",
    "* L1 regularization gives a sparse solution\n",
    "* L2 regularization is more computationally efficient\n",
    "* L1 regularization is robust to outliers\n",
    "* *Lasso regression* is linear regression with L1 regularization\n",
    "* \"Ridge regression\" is linear regression with L2 regularization\n",
    "\n",
    "Additional reading:\n",
    "* https://neptune.ai/blog/fighting-overfitting-with-l1-or-l2-regularization\n",
    "* https://scikit-learn.org/stable/modules/linear_model.html\n",
    "* https://scikit-learn.org/stable/modules/neural_networks_supervised.html#regularization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trick 2: Dropout\n",
    "\n",
    "Dropout \"injects\" noise by randomly dropping out (zeroing out) some of the nodes in each hidden layer at each round of training.\n",
    "\n",
    "Looking at the code in the book:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_layer(X, dropout):\n",
    "    assert 0 <= dropout <= 1\n",
    "    # this would drop out every node in this layer\n",
    "    if dropout == 1: \n",
    "        return torch.zeros_like(X)\n",
    "    # this drops out some nodes at random\n",
    "    mask = (torch.rand(X.shape) > dropout).float()\n",
    "    return mask * X / (1.0 - dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutMLPScratch(d2l.Classifier):\n",
    "    def __init__(self, num_outputs, num_hiddens_1, num_hiddens_2,\n",
    "                 dropout_1, dropout_2, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        # exercises for the curious:\n",
    "        #  (1) what is the architecture of this network?\n",
    "        #  (2) what is the computation graph of this network?\n",
    "        self.lin1 = nn.LazyLinear(num_hiddens_1)\n",
    "        self.lin2 = nn.LazyLinear(num_hiddens_2)\n",
    "        self.lin3 = nn.LazyLinear(num_outputs)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, X):\n",
    "        H1 = self.relu(self.lin1(X.reshape((X.shape[0], -1))))\n",
    "        # we drop out different nodes at random during each round of training, but never during evaluation or inference\n",
    "        if self.training:\n",
    "            H1 = dropout_layer(H1, self.dropout_1)\n",
    "        H2 = self.relu(self.lin2(H1))\n",
    "        if self.training:\n",
    "            H2 = dropout_layer(H2, self.dropout_2)\n",
    "        return self.lin3(H2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {'num_outputs':10, 'num_hiddens_1':256, 'num_hiddens_2':256,\n",
    "           'dropout_1':0.5, 'dropout_2':0.5, 'lr':0.1}\n",
    "model = DropoutMLPScratch(**hparams)\n",
    "data = d2l.FashionMNIST(batch_size=256)\n",
    "trainer = d2l.Trainer(max_epochs=10)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trick 3: Early Stopping\n",
    "\n",
    "So far, when we've said \"early stopping\" we've meant \"stop when the loss mostly stops changing\". Now we will talk about why this works, beyond the efficiency argument.\n",
    "\n",
    "When there is **label noise**, a neural network will fit clean data *first*. How do you know if there is label noise?\n",
    "* There may often naturally be label noise\n",
    "  * You could check if the same data point appears twice in the data with different labels\n",
    "  * You could look at any annotations on the data and see if there was annotator disagreement\n",
    "* It's also dead easy to *introduce* noise into the data\n",
    "\n",
    "Let's take a look at this fascinating paper about early stopping: http://proceedings.mlr.press/v139/garg21a/garg21a.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
