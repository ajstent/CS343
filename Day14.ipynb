{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers via Physics\n",
    "\n",
    "An optimizer is an algorithm for adjusting a model's weights toward a loss minimum.\n",
    "\n",
    "So let's think about this as rolling a ball (a weight) downhill (toward a loss minimum). \n",
    "\n",
    "According to physics, the potential energy $E$ of the ball $~ m*g*h$ (where $m$ is mass, $g$ is gravity and $h$ is height). \n",
    "\n",
    "And the force $F$ felt by the ball is $-\\triangledown E$ (the gradient of $E$).\n",
    "\n",
    "Also, by Newton's second law, $F = ma$ (where $a$ is acceleration). \n",
    "\n",
    "We will assume a mass of 1. Now $a = \\frac{\\Delta M}{\\Delta T}$ = $M(T) - M(T-1)$ ($M$ is momentum).\n",
    "\n",
    "So then $-\\triangledown E = F = M(T) - M(T-1)$.\n",
    "\n",
    "We know from regular stochastic descent that $-\\triangledown E = \\eta*dx$, so $M(T) = M(T-1)-\\eta*dx$.\n",
    "\n",
    "$M(T-1)$ is the velocity at the previous time. We want a *controlled* descent so we can add a coefficient to weight the degree to which the velocity at the previous time controls the velocity at this time (momentum): $M(T) = \\mu M(T-1)-\\eta*dx$. If $\\mu = 0$ we have regular SGD."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD with momentum\n",
    "\n",
    "For SGD with momentum, we:\n",
    "1. Use the gradient (acceleration) to update velocity via $M(T) = \\mu M(T-1)-\\eta*dx$.\n",
    "2. Use the velocity to update the position of the \"ball\" via $x(T) = x(T-1) + M(T)$.\n",
    "\n",
    "As you iterate over these steps, $\\mu$ accumulates exponentially.\n",
    "\n",
    "Common $\\mu$ values are 0.9 or 0.99."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam\n",
    "\n",
    "Adam builds on SGD with momentum.\n",
    "\n",
    "A problem with SGD with momentum is that the learning rate $\\eta$ affects all the weights equally, regardless of their individual gradients. By contrast, Adam adapts the learning rate dynamically per weight/bias.\n",
    "\n",
    "The intuition is that if $dx$ is small, we want to increase the learning rate; if $dx$ is large, we want to lower the learning rate.\n",
    "\n",
    "At the core of Adam is a modification to how we calculate $M(T)$: $M(T) = \\beta_1*M(T-1) + (1-\\beta_1)*dx$. We also define $M_t(T) = \\frac{M(T)}{1-\\beta_1^T}$.\n",
    "\n",
    "We want $\\beta_1$ to be a running average, so we can fix it to prefer newer vs older velocities. We also need to calculate a penalty term, $V(T) = \\beta_2*V(T-1) + (1-\\beta_2)*dx^2$. We also define $V_t(T) = \\frac{V(T)}{1-\\beta_2^T}$.\n",
    "\n",
    "And we adjust how we update the weights also: $x(T) = x(T-1)-\\frac{n*M_t(T)}{\\sqrt(V_t(T))+\\epsilon}$.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the algorithm and experimental results here: https://arxiv.org/pdf/1412.6980.pdf.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A problem with Adam is it may not converge when the moment estimates blow up. As an extension to the project, you could choose to implement Yogi (as outlined in the textbook)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
