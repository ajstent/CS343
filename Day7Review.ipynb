{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best practices when training and evaluating ML models using supervised learning\n",
    "\n",
    "1. Look at your data\n",
    "   * How many data points?\n",
    "   * Type of each data point?\n",
    "   * Missing or erroneous data?\n",
    "   * Relationships between data points (e.g. temporal, spatial)?\n",
    "   * Type of dependent variable: will task be regression, classification?\n",
    "   * If you can, get an idea of the accuracy of the labeling (e.g. interannotator agreement).\n",
    "   * Compute summary statistics on your data\n",
    "2. If necessary, transform or normalize your data\n",
    "   * Fill in missing values, or drop data points\n",
    "   * Correct erroneous values, or drop data points\n",
    "   * Consider scaling, translation, or rotation of data (*using what information from step 1?*)\n",
    "   * Consider data augmentation\n",
    "   * You may have to embed and/or encode your data\n",
    "3. Consider dimensionality reduction (*What are some methods for dimensionality reduction?*)\n",
    "   * PCA\n",
    "4. Determine how to split your data (*What are some ways?*)\n",
    "   * Random train/dev/test split\n",
    "   * Stratified sampling\n",
    "   * Temporal train/dev/test split\n",
    "   * k-fold cross-validation\n",
    "     * without replacement\n",
    "     * with replacement\n",
    "5. Select model architectures and define relevant items (*What do we need to define for each neural network architecture?*)\n",
    "   * number of hidden layers\n",
    "   * width of each hidden layer\n",
    "   * nature of connectedness\n",
    "   * activation function(s)\n",
    "   * loss function\n",
    "   * optimization algorithm\n",
    "6. Figure out the hyperparameters you will vary, and what the possible values or range for each will be (*What are some hyperparameters when using minibatch SGD as the optmization algorithm?*)\n",
    "  * learning rate\n",
    "  * number of epochs\n",
    "  * whether and how to regularize\n",
    "  * batch size\n",
    "7. Train and evaluate (for each model architecture and each combination of hyperparameters)\n",
    "  * As you evaluate, look for underfitting or overfitting\n",
    "8. Test on **held-out test data**\n",
    "9. Deploy and monitor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy and monitor: The ground is shifting beneath our feet\n",
    "\n",
    "Let's say you train a model on irises from 1998. However, now it's 2023 and:\n",
    "* natural selection may have led to changes in iris sizes\n",
    "* human selection has led to changes in iris sizes\n",
    "\n",
    "This is **covariate shift**.\n",
    "\n",
    "Or some biologist may have come along and done genetic testing on our irises, and discovered that some of them were assigned to the wrong species.\n",
    "\n",
    "This is **label shift**.\n",
    "\n",
    "Or, based on the genetic testing, the pesky biologist may have discovered that there's a fourth species of iris in our data - a new species.\n",
    "\n",
    "This is **concept shift**.\n",
    "\n",
    "The textbook covers some of the theory. Here's some practice:\n",
    "1. When you deploy your model, monitor its performance over time. This means continuing to label some data periodically, and the comparing your newly labeled data with your previously labeled data. You can discover all three types of drift in this way.\n",
    "2. If there has been shift, your model may have sufficient generalization to accommodate the shift, or it may not. Evaluate your model on your newly labeled data.\n",
    "3. If your model's performance has gotten worse, you may need to retrain your model. Many production models are retrained nightly (recommender systems) to quarterly or yearly (image classification, object detection, NLP)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Generalization, part two\n",
    "\n",
    "**Team exercise**: In your teams, answer these questions from the reading for today:\n",
    "1. If we wish to estimate the error of a fixed model to within 0.0001 with probability greater than 99.9%, how many samples do we need?\n",
    "2. Suppose that somebody else possesses a labeled test set and only makes available the unlabeled inputs (features). Now suppose that you can only access the test set labels by running a model (no restrictions placed on the model class) on each of the unlabeled inputs and receiving the corresponding error. How many models would you need to evaluate before you leak the entire test set and thus could appear to have error 0, regardless of your true error?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization, review\n",
    "\n",
    "We use an optimization algorithm to minimize the loss function for deep learning, and ultimately, to fit a function to the training data that we hope generalizes to test data. \n",
    "\n",
    "The function we are trying to fit, in deep learning, is typically *not* a line. What are some issues that arise when trying to fit different types of function?\n",
    "\n",
    "* local minima\n",
    "* saddle points\n",
    "* vanishing gradients\n",
    "\n",
    "How do we deal with these issues?\n",
    "\n",
    "* good parameter initialization\n",
    "* minibatch SGD\n",
    "* adjust the learning rate as the slope changes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
