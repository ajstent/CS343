{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz\n",
    "\n",
    "# Best practices when training and evaluating ML models using supervised learning\n",
    "\n",
    "Everything we will do after this point in the course is \"bells and whistles\". At the end of these first three weeks, you already have seen 80% of what you need to know to do deep learning well. Before we start making music, let's pause and consolidate the important stuff.\n",
    "\n",
    "**Team exercise**: complete this list with all the best practices you have learned.\n",
    "\n",
    "1. Look at your data (*What are some things you want to know about your data?*)\n",
    "   * thing 1\n",
    "   * thing 2\n",
    "   * thing 3\n",
    "   * thing 4\n",
    "   * thing 5\n",
    "   * thing 6\n",
    "   * thing 7\n",
    "2. If necessary, transform or normalize your data\n",
    "   * Fill in missing values, or drop data points\n",
    "   * Correct erroneous values, or drop data points\n",
    "   * Consider scaling, translation, or rotation of data (*using what information from step 1?*)\n",
    "   * Consider data augmentation\n",
    "   * You may have to embed and/or encode your data\n",
    "3. Consider dimensionality reduction (*What are some methods for dimensionality reduction?*)\n",
    "4. Determine how to split your data (*What are some ways?*)\n",
    "   * way 1\n",
    "   * way 2\n",
    "   * way 3\n",
    "   * way 4\n",
    "5. Select model architectures and define relevant items (*What do we need to define for each neural network architecture?*)\n",
    "   * item 1\n",
    "   * item 2\n",
    "   * item 3\n",
    "   * item 4\n",
    "   * item 5\n",
    "   * item 6\n",
    "6. Figure out the hyperparameters you will vary, and what the possible values or range for each will be (*What are some hyperparameters when using minibatch SGD as the optmization algorithm?*)\n",
    "   * hyperparameter 1\n",
    "   * hyperparameter 2\n",
    "   * hyperparameter 3\n",
    "   * hyperparameter 4\n",
    "7. Train and evaluate (for each model architecture and each combination of hyperparameters)\n",
    "   * As you evaluate, look for underfitting or overfitting\n",
    "     * underfitting is:\n",
    "     * overfitting is:\n",
    "8. Test on **held-out test data**\n",
    "9. Deploy and monitor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy and monitor: The ground is shifting beneath our feet\n",
    "\n",
    "Let's say you train a model on irises from 1998. However, now it's 2023 and:\n",
    "* natural selection may have led to changes in iris sizes\n",
    "* human selection has led to changes in iris sizes\n",
    "\n",
    "This is **covariate shift**.\n",
    "\n",
    "Or some biologist may have come along and done genetic testing on our irises, and discovered that some of them were assigned to the wrong species.\n",
    "\n",
    "This is **label shift**.\n",
    "\n",
    "Or, based on the genetic testing, the pesky biologist may have discovered that there's a fourth species of iris in our data - a new species.\n",
    "\n",
    "This is **concept shift**.\n",
    "\n",
    "The textbook covers some of the theory. Here's some practice:\n",
    "1. When you deploy your model, monitor its performance over time. This means continuing to label some data periodically, and the comparing your newly labeled data with your previously labeled data. You can discover all three types of drift in this way.\n",
    "2. If there has been shift, your model may have sufficient generalization to accommodate the shift, or it may not. Evaluate your model on your newly labeled data.\n",
    "3. If your model's performance has gotten worse, you may need to retrain your model. Many production models are retrained nightly (recommender systems) to quarterly or yearly (image classification, object detection, NLP)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Generalization, part two\n",
    "\n",
    "**Team exercise**: In your teams, answer these questions from the reading for today:\n",
    "1. If we wish to estimate the error of a fixed model to within 0.0001 with probability greater than 99.9%, how many samples do we need?\n",
    "2. Suppose that somebody else possesses a labeled test set and only makes available the unlabeled inputs (features). Now suppose that you can only access the test set labels by running a model (no restrictions placed on the model class) on each of the unlabeled inputs and receiving the corresponding error. How many models would you need to evaluate before you leak the entire test set and thus could appear to have error 0, regardless of your true error?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization, review\n",
    "\n",
    "We use an optimization algorithm to minimize the loss function for deep learning, and ultimately, to fit a function to the training data that we hope generalizes to test data. \n",
    "\n",
    "The function we are trying to fit, in deep learning, is typically *not* a line. What are some issues that arise when trying to fit different types of function?\n",
    "* issue 1\n",
    "* issue 2\n",
    "* issue 3\n",
    "\n",
    "How do we deal with these issues?\n",
    "* method 1\n",
    "* method 2\n",
    "* method 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
