{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation through time\n",
    "\n",
    "## RNN review\n",
    "\n",
    "*A RNN with a single hidden layer with ReLU activation, and an output layer with softmax, is defined by what?*\n",
    "1. item 1\n",
    "2. item 2\n",
    "3. item 3\n",
    "4. item 4\n",
    "5. item 5\n",
    "\n",
    "## One more step!\n",
    "\n",
    "*A RNN with two hidden layers, each with ReLU activation, and an output layer with softmax, is defined by what?*\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation in a MLP\n",
    "\n",
    "A MLP looks quite similar to a RNN. Imagine a MLP with one hidden layer with ReLU activation, and an output layer with softmax. Bring up your implementation of backpropagation from project 2. Here's the forward pass:\n",
    "1. y_netIn = X@y_wts + y_b\n",
    "2. y_netAct = 1 / (1 + exp(-y_netIn)\n",
    "3. z_netIn = y_netAct@z_wts + z_b\n",
    "4. z_netAct = 1 / (1 + exp(-z_netIn)\n",
    "5. loss (cross entropy)\n",
    "\n",
    "What are the derivatives you calculated?\n",
    "\n",
    "1. dz_netAct (meets loss from above): \n",
    "2. Three parts!\n",
    "   * dz_netIn (meets dz_netAct from above and what from below?)\n",
    "   * dz_wts (meets dz_netIn from above and what from below?)\n",
    "   * dz_b (meets dz_netIn from above and what from below?)\n",
    "3. dy_netAct (meets dz_netIn from above and what from below?)\n",
    "4. Three parts!\n",
    "   * dy_netIn (meets dy_netAct from above and what from below?)\n",
    "   * dy_wts\n",
    "   * dy_b"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation in a RNN\n",
    "\n",
    "Let's stick with a one hidden layer RNN defined as before. Here's the forward pass:\n",
    "\n",
    "1. $h_t = W_{xh} x_t + w_{hh} h_{t-1} + b_h$\n",
    "2. $o_t = W_{qh} h_t + b_q$\n",
    "\n",
    "Loss is $1/T \\sum_{t=1}^T l(o_t, y_t)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for backprop, we need to calculate:\n",
    "\n",
    "1. $dy$ (analogous to dz_netAct)\n",
    "2. Two parts!\n",
    "   * $dW_{qh}$ (meets what from above and what from below?)\n",
    "   * $db_{q}$ (meets what from above and what from below?)\n",
    "2. $dh$ (analogous to dy_netAct)\n",
    "3. Three parts!\n",
    "   * $dW_{hh}$ (meets what from above and what from below?)\n",
    "   * $dW_{xh}$ (meets what from above and what from below?)\n",
    "   * $db_h$ (meets what from above and what from below?)\n",
    "\n",
    "Take some time and define these."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*How would you handle two hidden layers?*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with long histories\n",
    "\n",
    "The textbook outlines three strategies:\n",
    "1. Full computation\n",
    "2. Regular truncation\n",
    "3. Random truncation\n",
    "\n",
    "Let's talk about each."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Karpathy again!\n",
    "\n",
    "http://karpathy.github.io/2015/05/21/rnn-effectiveness/"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
