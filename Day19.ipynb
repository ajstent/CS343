{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation through time\n",
    "\n",
    "## RNN review\n",
    "\n",
    "*A RNN with a single hidden layer with ReLU activation, and an output layer with softmax, is defined by what?*\n",
    "1. $W_{xh}$\n",
    "2. $W_{hh}$\n",
    "3. $b_h$\n",
    "4. $W_{hq}$\n",
    "5. $b_q$\n",
    "\n",
    "In particular, the equivalent of y_netIn ~ $W_{xh}*x_t + W_{hh}*h_{t-1} + b_h$.\n",
    "\n",
    "And the equivalent of y_netAct ~ $\\phi(W_{xh}*x_t + W_{hh}*h_{t-1} + b_h)$.\n",
    "\n",
    "And the equivalent of z_netIn ~ $W_{hq}*y_netAct + b_q$.\n",
    "\n",
    "## One more step!\n",
    "\n",
    "*A RNN with two hidden layers, each with ReLU activation, and an output layer with softmax, is defined by what?*\n",
    "\n",
    "1. $W_{xh1}$\n",
    "2. $W_{hh1}$\n",
    "3. $b_{h1}$\n",
    "4. $W_{xh2}$\n",
    "5. $W_{hh2}$\n",
    "6. $b_{h2}$\n",
    "7. $W_{hq}$\n",
    "8. $b_q$\n",
    "\n",
    "And we will be tracking two hidden states, one for each hidden layer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation in a MLP\n",
    "\n",
    "A MLP looks quite similar to a RNN. Imagine a MLP with one hidden layer with ReLU activation, and an output layer with softmax. Bring up your implementation of backpropagation from project 2. Here's the forward pass:\n",
    "1. y_netIn = X@y_wts + y_b\n",
    "2. y_netAct = max(y_netIn, 0)\n",
    "3. z_netIn = y_netAct@z_wts + z_b\n",
    "4. z_netAct = 1 / (1 + exp(-z_netIn))\n",
    "5. loss (cross entropy)\n",
    "\n",
    "What are the derivatives you calculated?\n",
    "\n",
    "1. dz_netAct (meets loss from above and z_netIn from below): -1/(len(z_netAct)*z_netAct) (derivative of cross entropy loss)\n",
    "2. Three parts!\n",
    "   * dz_netIn (meets dz_netAct from above and z_wts from below): dz_netAct * (one_hot(y, num_outputs) - z_netAct)\n",
    "   * dz_wts (meets dz_netIn from above and y_netAct): (dz_netIn.T@y_netAct).T + reg*z_wts\n",
    "   * dz_b (meets dz_netIn from above): sum(dz_netIn, axis=0)\n",
    "3. dy_netAct (meets dz_netIn from above and dy_netIn from below): dz_netIn@z_wts.T\n",
    "4. Three parts!\n",
    "   * dy_netIn (meets dy_netAct from above and y_wts from below): dy_netAct*(np.where(y_netIn <= 0, 0, 1))\n",
    "   * dy_wts: (dy_netIn.T@features).T + reg*y_wts\n",
    "   * dy_b: sum(dy_netIn, axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation in a RNN\n",
    "\n",
    "Let's stick with a one hidden layer RNN defined as before. Here's the forward pass:\n",
    "\n",
    "1. $h_t = W_{xh} x_t + w_{hh} h_{t-1} + b_h$\n",
    "2. $o_t = W_{hq} h_t + b_q$\n",
    "\n",
    "Loss is $1/T \\sum_{t=1}^T l(o_t, y_t)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for backprop, we need to calculate:\n",
    "\n",
    "1. $dy$ (analogous to dz_netAct): -1/(len(y_t)*y_t)\n",
    "2. Two parts!\n",
    "   * $dW_{hq}$ (meets dy from above and the output from h from below, which we will call h_netAct): dy_t@h_netAct_t.T\n",
    "   * $db_{q}$ (meets dy from above and nothing from below): dy_t\n",
    "2. $dh_t$ (analogous to dy_netAct): $W_{hq}$.T@$dy_t$ + $h_t$, **and then backprop through the activation whether tanh or ReLU to get dhraw_t**\n",
    "3. Three parts!\n",
    "   * $dW_{hh}$ (meets dh from above and $h_{t-1}$): $dhraw_t$@$h_{t-1}$\n",
    "   * $dW_{xh}$ (meets dh from above and $x_t$): $dhraw_t$@$x_t$\n",
    "   * $db_h$ (meets dh from above): $dhraw_t$\n",
    "\n",
    "Take some time and define these."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*How would you handle two hidden layers?*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with long histories\n",
    "\n",
    "The textbook outlines three strategies:\n",
    "1. Full computation\n",
    "2. Regular truncation\n",
    "3. Random truncation\n",
    "\n",
    "Let's talk about each."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Karpathy again!\n",
    "\n",
    "http://karpathy.github.io/2015/05/21/rnn-effectiveness/"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
