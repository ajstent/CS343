{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Stacks of Convolution \n",
        "\n",
        "Welcome to Day 16! My name is Tahiya Chowdhury, a Postdoctoral Research Fellow at Davis Institute for AI at Colby.\n",
        "\n",
        "We met earlier this semester when we talked about Multi-layer perceptron. Later, we learned about more advanced and powerful neural networks called Convolutional Neural Network (CNN). \n",
        "\n",
        "CNNs have the state of the art for decades particularly for **computer vision** dealing with images, but equally effective for audio, text, and time series such as EEG or sensor signals.\n",
        "\n",
        "The first CNN developed for Handwritten digit recognition task, called **LeNet**, which we have already seen in Day 13. Over the years, a wide variants of CNN architectures emerged from different design choices in search of:\n",
        "* practical and feasible for solving vision tasks (padding, pooling, regularization)\n",
        "* Extending existing networks for more complex, specialized tasks \n",
        "    * segmentation ([U-Net](https://arxiv.org/pdf/1505.04597.pdf))\n",
        "    * detection ([R-CNN](https://arxiv.org/abs/1703.06870)) \n",
        "    * tracking (you probably have heard of object tracking, how about [particle tracking](https://www.pnas.org/doi/10.1073/pnas.1804420115)?)"
      ],
      "metadata": {
        "id": "KuulJtEfHY41"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A lot of the earlier architectures and techniques still appear useful when designing modern networks \n",
        "* U-Net, developed biomedical images, are often used in diffusion model (yes, [Stable diffusion](https://replicate.com/stability-ai/stable-diffusion) is one such diffusion model)\n",
        "* Padding is a useful technique for Transformers, a state of the art technique in many machine learning tasks\n",
        "\n",
        "The motivation for Day 16 is to understand how classic convolutional operation, when coupled with many **layers** of **stacked convolution** operations, can be very powerful to learn complex features and build deeper models for classification and other tasks.\n",
        "\n",
        "The textbook talks about the several Deep Convolutional Neural Networks that slowly replaced classical computer vision pipeline that relied on pre-calculated features instead of learning the features (representation) during classifier training.\n"
      ],
      "metadata": {
        "id": "FDvlzNyiHheA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Can we think of a limitation of models built on pre-calculated hand-crafted features?"
      ],
      "metadata": {
        "id": "1T1s3Z5fgoBa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Today we will look at AlexNet and ResNet\n",
        "\n",
        "### [AlexNet](https://papers.nips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) (Krizhevsky et al.)\n",
        "\n"
      ],
      "metadata": {
        "id": "QFqrhfIBHpiX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quick facts about AlexNet**\n",
        "\n",
        "Input: 224 X 224 X 3 channel image\n",
        "\n",
        "++\n",
        "* Data: 1 million images from ImageNet\n",
        "* Data augmentation used to instead traning data size\n",
        "* ReLU non-linear activation to avoid vanishing gradient (gradient decreasing in deep networks due to small derivative in backpropagation)\n",
        "\n",
        "--\n",
        "* Large number of learnable parameters in optimization (LeNet: ~60000 parameters)\n",
        "* high convergence time due to normalization and dropout to avoid overfitting\n",
        "\n",
        "We will use [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset available from PyTorch for this example.\n",
        "\n",
        "This example is based on [PaperSpace](https://blog.paperspace.com) DL models from scratch blogseries."
      ],
      "metadata": {
        "id": "pCENYmGHHr45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import packages"
      ],
      "metadata": {
        "id": "lggI42tEHvot"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PgutAcrxCf0z"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's set device configuration to use GPU if available (because AlexNet has 62 million trainable parameters!)"
      ],
      "metadata": {
        "id": "lc9j4Kw5HpI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "4Q37ssGICkrn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We define a train-validation loader and test loader for batch loading the data"
      ],
      "metadata": {
        "id": "QR5uXfJsJDUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_valid_loader(data_dir,\n",
        "                           batch_size,\n",
        "                           augment,\n",
        "                           random_seed,\n",
        "                           valid_size=0.1,\n",
        "                           shuffle=True):\n",
        "\n",
        "    #normalizing function usimng mean and standard deviation, will be applied to each channel sepaartely\n",
        "    normalize = transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010],)\n",
        "\n",
        "    # define transforms\n",
        "    valid_transform = transforms.Compose([\n",
        "            transforms.Resize((227,227)),\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "    ])\n",
        "\n",
        "  # If augmentation is used to increase training data\n",
        "  # two augmentation is considered: Crop and Flip\n",
        "  # Original AlexNet uses a third augmentation which is omitted \n",
        "    if augment:\n",
        "        train_transform = transforms.Compose([\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ])\n",
        "\n",
        "    else:\n",
        "        train_transform = transforms.Compose([\n",
        "            transforms.Resize((227,227)),\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ])\n",
        "\n",
        "    # load the dataset: train\n",
        "    train_dataset = datasets.CIFAR10(\n",
        "        root=data_dir, train=True,\n",
        "        download=True, transform=train_transform,\n",
        "    )\n",
        "\n",
        "    # load the dataset: valid\n",
        "    valid_dataset = datasets.CIFAR10(\n",
        "        root=data_dir, train=True,\n",
        "        download=True, transform=valid_transform,\n",
        "    )\n",
        "\n",
        "    # train and valid split\n",
        "    num_train = len(train_dataset)\n",
        "    indices = list(range(num_train))\n",
        "    split = int(np.floor(valid_size * num_train))\n",
        "\n",
        "    # random shuffling for generalization\n",
        "    if shuffle:\n",
        "        np.random.seed(random_seed)\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "    train_idx, valid_idx = indices[split:], indices[:split]\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "    #batch loading data using random sampler\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
        " \n",
        "    valid_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset, batch_size=batch_size, sampler=valid_sampler)\n",
        "\n",
        "    return (train_loader, valid_loader)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDYoXSLjF7X6",
        "outputId": "6d437a9d-7855-4172-e0a0-bc8acd1738bb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:04<00:00, 41054639.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test loader\n",
        "def get_test_loader(data_dir,\n",
        "                    batch_size,\n",
        "                    shuffle=True):\n",
        "    normalize = transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225],\n",
        "    )\n",
        "\n",
        "    # define transform\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((227,227)),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "    ])\n",
        "\n",
        "    dataset = datasets.CIFAR10(\n",
        "        root=data_dir, train=False,\n",
        "        download=True, transform=transform,\n",
        "    )\n",
        "\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=batch_size, shuffle=shuffle\n",
        "    )\n",
        "\n",
        "    return data_loader\n",
        "\n",
        "# MNIST dataset loading\n",
        "train_loader, valid_loader = get_train_valid_loader(data_dir = './data', batch_size = 64,\n",
        "                       augment = False, random_seed = 1)\n",
        "\n",
        "test_loader = get_test_loader(data_dir = './data',\n",
        "                              batch_size = 64)"
      ],
      "metadata": {
        "id": "uQqN7M9iLDhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's check the split of the data"
      ],
      "metadata": {
        "id": "gJANgPTELw6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of train images:\", len(train_loader)*64)  #45000 images\n",
        "print(\"Number of train images:\", len(valid_loader)*64)  #5000 images\n",
        "print(\"Number of train images:\", len(test_loader)*64)   #10000 image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Heo2bBUXGltZ",
        "outputId": "d077201e-ed7f-498a-a7c5-149c55696631"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of train images: 45056\n",
            "Number of train images: 5056\n",
            "Number of train images: 10048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions\n",
        "\n",
        "* Why is `shuffle` used in the loader function?\n",
        "* The input dimension is 227 X 227. Can we use input with other sizes here?\n",
        "* For what purpose, augmentation is used here?\n",
        "* How does normalization help in training?"
      ],
      "metadata": {
        "id": "oe-Qbz5JnLbo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now we will implement the main architecture of AlexNet"
      ],
      "metadata": {
        "id": "H88E1WhdMT60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#AlexNet\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=0),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(384),\n",
        "            nn.ReLU())\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(384),\n",
        "            nn.ReLU())\n",
        "        self.layer5 = nn.Sequential(\n",
        "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(9216, 4096),\n",
        "            nn.ReLU())\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU())\n",
        "        self.fc2= nn.Sequential(\n",
        "            nn.Linear(4096, num_classes))\n",
        "\n",
        "    #putting all the layers together   \n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.layer5(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        out = self.fc1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "gtA39slUF9mQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training AlexNet"
      ],
      "metadata": {
        "id": "0DfVXateNMvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 10\n",
        "num_epochs = 20\n",
        "batch_size = 64\n",
        "learning_rate = 0.005\n",
        "\n",
        "model = AlexNet(num_classes).to(device)\n",
        "\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)  \n",
        "\n",
        "\n",
        "# Train the model\n",
        "total_step = len(train_loader)"
      ],
      "metadata": {
        "id": "fNDvdPokGGWD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_step = len(train_loader)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):  \n",
        "        # Move tensors to the configured device\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "            \n",
        "    # Validation\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in valid_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            del images, labels, outputs\n",
        "    \n",
        "        print('Accuracy of the network on the {} validation images: {} %'.format(5000, 100 * correct / total)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMyRXsx3GIgI",
        "outputId": "c7773b70-bf0c-41d4-8943-277588acea55"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Step [704/704], Loss: 1.5783\n",
            "Accuracy of the network on the 5000 validation images: 58.58 %\n",
            "Epoch [2/20], Step [704/704], Loss: 0.6471\n",
            "Accuracy of the network on the 5000 validation images: 69.16 %\n",
            "Epoch [3/20], Step [704/704], Loss: 1.4001\n",
            "Accuracy of the network on the 5000 validation images: 70.24 %\n",
            "Epoch [4/20], Step [704/704], Loss: 0.9505\n",
            "Accuracy of the network on the 5000 validation images: 74.06 %\n",
            "Epoch [5/20], Step [704/704], Loss: 0.9067\n",
            "Accuracy of the network on the 5000 validation images: 77.64 %\n",
            "Epoch [6/20], Step [704/704], Loss: 1.4402\n",
            "Accuracy of the network on the 5000 validation images: 77.3 %\n",
            "Epoch [7/20], Step [704/704], Loss: 0.4090\n",
            "Accuracy of the network on the 5000 validation images: 78.46 %\n",
            "Epoch [8/20], Step [704/704], Loss: 0.3330\n",
            "Accuracy of the network on the 5000 validation images: 79.26 %\n",
            "Epoch [9/20], Step [704/704], Loss: 0.2193\n",
            "Accuracy of the network on the 5000 validation images: 80.34 %\n",
            "Epoch [10/20], Step [704/704], Loss: 1.3074\n",
            "Accuracy of the network on the 5000 validation images: 79.92 %\n",
            "Epoch [11/20], Step [704/704], Loss: 0.8712\n",
            "Accuracy of the network on the 5000 validation images: 81.44 %\n",
            "Epoch [12/20], Step [704/704], Loss: 1.0555\n",
            "Accuracy of the network on the 5000 validation images: 81.1 %\n",
            "Epoch [13/20], Step [704/704], Loss: 1.1576\n",
            "Accuracy of the network on the 5000 validation images: 81.32 %\n",
            "Epoch [14/20], Step [704/704], Loss: 0.7040\n",
            "Accuracy of the network on the 5000 validation images: 81.7 %\n",
            "Epoch [15/20], Step [704/704], Loss: 0.0561\n",
            "Accuracy of the network on the 5000 validation images: 81.86 %\n",
            "Epoch [16/20], Step [704/704], Loss: 0.6413\n",
            "Accuracy of the network on the 5000 validation images: 81.88 %\n",
            "Epoch [17/20], Step [704/704], Loss: 0.7961\n",
            "Accuracy of the network on the 5000 validation images: 81.7 %\n",
            "Epoch [18/20], Step [704/704], Loss: 0.5794\n",
            "Accuracy of the network on the 5000 validation images: 80.9 %\n",
            "Epoch [19/20], Step [704/704], Loss: 0.4434\n",
            "Accuracy of the network on the 5000 validation images: 83.34 %\n",
            "Epoch [20/20], Step [704/704], Loss: 1.0974\n",
            "Accuracy of the network on the 5000 validation images: 79.76 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing on Unseen data"
      ],
      "metadata": {
        "id": "xxhPBunePl6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        del images, labels, outputs\n",
        "\n",
        "    print('Accuracy of the network on the {} test images: {} %'.format(10000, 100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jeEvX0MGLXY",
        "outputId": "1a76adad-db54-41a3-d9c0-5a9922ce7c9a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 79.36 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions\n",
        "\n",
        "* Why augmentation is only used on training\n",
        "\n",
        "* There are lots of choices about the parameters here! Feel free to experiment with one of them.\n",
        "  * Try changing the kernel size. Do you observe any change?\n",
        "\n",
        "  * What about changing the stride?\n",
        "\n",
        "  * Change the optimizer from SGD to Adam.\n",
        "\n",
        "* What is weight decay? \n",
        "  * It is regularization technique to avoid overfitting by penalizing large weight by a weghting factor\n",
        "\n",
        "* What is momentum?\n",
        "\n",
        "* Observe the loss and accuracy for the 20 epochs. What can you see? How do you explain it?\n",
        "\n",
        "* You are curious to observe how loss and accuracy changes per epoch visually? How would you record these to plot them?\n",
        "\n",
        "* For calculating classification performance, this example used accuracy. Are there other metrics? Is accuracy the correct metric to use in this case? "
      ],
      "metadata": {
        "id": "bUuqZU79HX3x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Optional] Let's look at [ResNet](https://arxiv.org/pdf/1512.03385.pdf) (He et al.)"
      ],
      "metadata": {
        "id": "oCQYX4BtVT1p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quick facts about ResNet**\n",
        "\n",
        "Input: 224 X 224 X 3 channel image\n",
        "\n",
        "++\n",
        "* Data: 1 million images from ImageNet\n",
        "* Bigger network with many layers, but fewer trainable parameters\n",
        "* Skip connection to reduce number of parameters and avoid vanishing gradients\n",
        "\n",
        "\n",
        "We will use [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset available from PyTorch for this example."
      ],
      "metadata": {
        "id": "T08GD5YvHX6Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining data loader"
      ],
      "metadata": {
        "id": "e9kT8Hmvpvqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_loader(data_dir,\n",
        "                batch_size,\n",
        "                random_seed=42,\n",
        "                valid_size=0.1,\n",
        "                shuffle=True,\n",
        "                test=False):\n",
        "  \n",
        "    normalize = transforms.Normalize(\n",
        "        mean=[0.4914, 0.4822, 0.4465],\n",
        "        std=[0.2023, 0.1994, 0.2010],\n",
        "    )\n",
        "\n",
        "    # define transforms\n",
        "    transform = transforms.Compose([\n",
        "            transforms.Resize((224,224)),\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "    ])\n",
        "\n",
        "    if test:\n",
        "        dataset = datasets.CIFAR10(\n",
        "          root=data_dir, train=False,\n",
        "          download=True, transform=transform,\n",
        "        )\n",
        "\n",
        "        data_loader = torch.utils.data.DataLoader(\n",
        "            dataset, batch_size=batch_size, shuffle=shuffle\n",
        "        )\n",
        "\n",
        "        return data_loader\n",
        "\n",
        "    # load the dataset\n",
        "    train_dataset = datasets.CIFAR10(\n",
        "        root=data_dir, train=True,\n",
        "        download=True, transform=transform,\n",
        "    )\n",
        "\n",
        "    valid_dataset = datasets.CIFAR10(\n",
        "        root=data_dir, train=True,\n",
        "        download=True, transform=transform,\n",
        "    )\n",
        "\n",
        "    num_train = len(train_dataset)\n",
        "    indices = list(range(num_train))\n",
        "    split = int(np.floor(valid_size * num_train))\n",
        "\n",
        "    if shuffle:\n",
        "        np.random.seed(42)\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "    train_idx, valid_idx = indices[split:], indices[:split]\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
        " \n",
        "    valid_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset, batch_size=batch_size, sampler=valid_sampler)\n",
        "\n",
        "    return (train_loader, valid_loader)\n",
        "\n",
        "\n",
        "# CIFAR10 dataset \n",
        "train_loader, valid_loader = data_loader(data_dir='./data',\n",
        "                                         batch_size=64)\n",
        "\n",
        "test_loader = data_loader(data_dir='./data',\n",
        "                              batch_size=64,\n",
        "                              test=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaHLOvy-Ypq8",
        "outputId": "2a49c7ae-79d5-419d-b330-099c04ce3214"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the Residual Block"
      ],
      "metadata": {
        "id": "LxJ7uxIIp9KN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride = 1, downsample = None):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "                        nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = stride, padding = 1),\n",
        "                        nn.BatchNorm2d(out_channels),\n",
        "                        nn.ReLU())\n",
        "        self.conv2 = nn.Sequential(\n",
        "                        nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1),\n",
        "                        nn.BatchNorm2d(out_channels))\n",
        "        self.downsample = downsample\n",
        "        self.relu = nn.ReLU()\n",
        "        self.out_channels = out_channels\n",
        "        \n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.conv2(out)\n",
        "        if self.downsample:\n",
        "            residual = self.downsample(x)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "zcSfKneqY6c7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet architecture"
      ],
      "metadata": {
        "id": "0g3J0DylqNbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes = 10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.inplanes = 64\n",
        "        self.conv1 = nn.Sequential(\n",
        "                        nn.Conv2d(3, 64, kernel_size = 7, stride = 2, padding = 3),\n",
        "                        nn.BatchNorm2d(64),\n",
        "                        nn.ReLU())\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
        "        self.layer0 = self._make_layer(block, 64, layers[0], stride = 1)\n",
        "        self.layer1 = self._make_layer(block, 128, layers[1], stride = 2)\n",
        "        self.layer2 = self._make_layer(block, 256, layers[2], stride = 2)\n",
        "        self.layer3 = self._make_layer(block, 512, layers[3], stride = 2)\n",
        "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "        \n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes:\n",
        "            \n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes, kernel_size=1, stride=stride),\n",
        "                nn.BatchNorm2d(planes),\n",
        "            )\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "    \n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer0(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "YDdWdNqZZJBS"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining Hyperparameters"
      ],
      "metadata": {
        "id": "zrcyqbhBqRC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 10\n",
        "num_epochs = 20\n",
        "batch_size = 16\n",
        "learning_rate = 0.01\n",
        "\n",
        "model = ResNet(ResidualBlock, [3, 4, 6, 3]).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.001, momentum = 0.9)  \n",
        "\n",
        "# Train the model\n",
        "total_step = len(train_loader)"
      ],
      "metadata": {
        "id": "cr4QdNDJZO9C"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training ResNet"
      ],
      "metadata": {
        "id": "qwW0bxW9qUbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "total_step = len(train_loader)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):  \n",
        "        # Move tensors to the configured device\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        del images, labels, outputs\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    print ('Epoch [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, loss.item()))\n",
        "            \n",
        "    # Validation\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in valid_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            del images, labels, outputs\n",
        "    \n",
        "        print('Accuracy of the network on the {} validation images: {} %'.format(5000, 100 * correct / total)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qeyaj0ADZUSc",
        "outputId": "8edf7b92-74df-4fc7-eec3-99c4ee36d1b9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Loss: 1.9688\n",
            "Accuracy of the network on the 5000 validation images: 55.36 %\n",
            "Epoch [2/20], Loss: 1.3953\n",
            "Accuracy of the network on the 5000 validation images: 71.12 %\n",
            "Epoch [3/20], Loss: 1.0717\n",
            "Accuracy of the network on the 5000 validation images: 77.2 %\n",
            "Epoch [4/20], Loss: 1.1225\n",
            "Accuracy of the network on the 5000 validation images: 78.96 %\n",
            "Epoch [5/20], Loss: 0.2865\n",
            "Accuracy of the network on the 5000 validation images: 79.26 %\n",
            "Epoch [6/20], Loss: 0.5070\n",
            "Accuracy of the network on the 5000 validation images: 81.4 %\n",
            "Epoch [7/20], Loss: 0.9793\n",
            "Accuracy of the network on the 5000 validation images: 81.68 %\n",
            "Epoch [8/20], Loss: 0.2860\n",
            "Accuracy of the network on the 5000 validation images: 83.48 %\n",
            "Epoch [9/20], Loss: 0.2861\n",
            "Accuracy of the network on the 5000 validation images: 82.06 %\n",
            "Epoch [10/20], Loss: 0.2897\n",
            "Accuracy of the network on the 5000 validation images: 83.12 %\n",
            "Epoch [11/20], Loss: 0.2443\n",
            "Accuracy of the network on the 5000 validation images: 82.32 %\n",
            "Epoch [12/20], Loss: 0.6190\n",
            "Accuracy of the network on the 5000 validation images: 82.96 %\n",
            "Epoch [13/20], Loss: 0.8051\n",
            "Accuracy of the network on the 5000 validation images: 82.4 %\n",
            "Epoch [14/20], Loss: 0.5422\n",
            "Accuracy of the network on the 5000 validation images: 82.6 %\n",
            "Epoch [15/20], Loss: 0.1444\n",
            "Accuracy of the network on the 5000 validation images: 83.16 %\n",
            "Epoch [16/20], Loss: 0.4691\n",
            "Accuracy of the network on the 5000 validation images: 83.12 %\n",
            "Epoch [17/20], Loss: 0.2292\n",
            "Accuracy of the network on the 5000 validation images: 83.26 %\n",
            "Epoch [18/20], Loss: 0.0566\n",
            "Accuracy of the network on the 5000 validation images: 83.66 %\n",
            "Epoch [19/20], Loss: 0.0073\n",
            "Accuracy of the network on the 5000 validation images: 83.98 %\n",
            "Epoch [20/20], Loss: 0.3757\n",
            "Accuracy of the network on the 5000 validation images: 82.6 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test on unseen test images"
      ],
      "metadata": {
        "id": "a-iL7cwQqXQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        del images, labels, outputs\n",
        "\n",
        "    print('Accuracy of the network on the {} test images: {} %'.format(10000, 100 * correct / total))   "
      ],
      "metadata": {
        "id": "m5dY3EHyZYWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions\n",
        "\n",
        "* What characteristics do you observe from the two architectures?\n",
        "  * similarities\n",
        "  * differences"
      ],
      "metadata": {
        "id": "LdldEPHlqap9"
      }
    }
  ]
}