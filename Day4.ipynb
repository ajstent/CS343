{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That TensorBoard playground\n",
    "\n",
    "Who will win?\n",
    "\n",
    "https://playground.tensorflow.org/\n",
    "\n",
    "The simple dataset in the bottom left, and **no hidden layers** to your network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "* What does a linear regression look like (function)? $f(x) = X\\vec{w} + b$ or $y = w_1 x_1 + w_2 x_2 + ... + b$\n",
    "* When we fit a linear regression using a simple neural network:\n",
    "  * What is the width of the input layer? for $n$ features, $n + 1$ (+ 1 for the bias)\n",
    "  * What is the width of the output layer? 1 (because simple linear regression)\n",
    "  * What type of neural network is it, in terms of connections? feedforward\n",
    "  * What are the parameters? the weights and the bias\n",
    "  * What does the loss function look like (function)? MSSE, or $1/n\\sum_{i=1}^N 1/2(y-\\hat{y})^2$ (could do exp 4, 6 etc or || but not odd valued exponents because sign shouldn't matter)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Dive on Gradient Descent\n",
    "\n",
    "First, we install and import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision\n",
    "!pip install d2l==1.0.0b0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alex Strick van Linschoten has a great overview of [gradient descent](https://mlops.systems/posts/2022-05-12-seven-steps-gradient-calculations.html). Let's take a look.\n",
    "1. Initialise a set of weights\n",
    "2. Use the weights to make a prediction\n",
    "3. Loss: see how well we did with our predictions\n",
    "4. Calculate the gradients across all the weights\n",
    "5. ‘Step’: Update the weights\n",
    "6. Repeat starting at step 2\n",
    "7. Iterate until we decide to stop\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this in mind, let's take a deeper look at those Module and Trainer classes we have been subclassing.\n",
    "\n",
    "1. Annotate (comment) these classes with the location of each of these seven staps in basic gradient descent.\n",
    "\n",
    "2. Annotate the code that implements the *stochastic* part of what we are doing.\n",
    "\n",
    "3. Annotate the code that implements *minibatch*. (What are the alternatives to minibatch that we have discussed so far?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(d2l.HyperParameters):  \n",
    "    \"\"\"The base class for training models with data. From https://d2l.ai/chapter_linear-regression/oo-design.html. Augmented with prepare_batch and fit_epoch for minibatch SGD.\"\"\"\n",
    "    def __init__(self, max_epochs, num_gpus=0, gradient_clip_val=0):\n",
    "        self.save_hyperparameters()\n",
    "        assert num_gpus == 0, 'No GPU support yet'\n",
    "\n",
    "    def prepare_data(self, data):\n",
    "        # pytorch defines a dataloader class\n",
    "        self.train_dataloader = data.train_dataloader()\n",
    "        self.val_dataloader = data.val_dataloader()\n",
    "        self.num_train_batches = len(self.train_dataloader)\n",
    "        self.num_val_batches = (len(self.val_dataloader)\n",
    "                                if self.val_dataloader is not None else 0)\n",
    "\n",
    "    def prepare_model(self, model):\n",
    "        # a model has a trainer and a trainer has a model\n",
    "        model.trainer = self\n",
    "        # set up the plot\n",
    "        model.board.xlim = [0, self.max_epochs]\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, model, data):\n",
    "        # to fit, we need a model and data\n",
    "        self.prepare_data(data)\n",
    "        self.prepare_model(model)\n",
    "        self.optim = model.configure_optimizers()\n",
    "        self.epoch = 0\n",
    "        self.train_batch_idx = 0\n",
    "        self.val_batch_idx = 0\n",
    "        # Step 6 and Step 7\n",
    "        # no early stopping\n",
    "        for self.epoch in range(self.max_epochs):\n",
    "            # truly stochastic: shuffle the training data\n",
    "            self.fit_epoch()\n",
    "            # early stopping: if loss hasn't changed return\n",
    "\n",
    "    def prepare_batch(self, batch):\n",
    "        return batch\n",
    "\n",
    "    def fit_epoch(self):\n",
    "        self.model.train()\n",
    "        # Minibatch\n",
    "        for batch in self.train_dataloader:\n",
    "            loss = self.model.training_step(self.prepare_batch(batch))\n",
    "            self.optim.zero_grad()\n",
    "            with torch.no_grad():\n",
    "                # Step 4 happens in backward; also Step 5\n",
    "                loss.backward()\n",
    "                if self.gradient_clip_val > 0:  # To be discussed later\n",
    "                    self.clip_gradients(self.gradient_clip_val, self.model)\n",
    "                self.optim.step()\n",
    "            self.train_batch_idx += 1\n",
    "        # validation (or testing)\n",
    "        if self.val_dataloader is None:\n",
    "            return\n",
    "        self.model.eval()\n",
    "        for batch in self.val_dataloader:\n",
    "            with torch.no_grad():\n",
    "                self.model.validation_step(self.prepare_batch(batch))\n",
    "            self.val_batch_idx += 1\n",
    "\n",
    "class SGD(d2l.HyperParameters):  #@save\n",
    "    \"Our SGD class, from https://d2l.ai/chapter_linear-regression/linear-regression-scratch.html.\"\n",
    "    def __init__(self, params, lr):\n",
    "        \"\"\"Minibatch stochastic gradient descent.\"\"\"\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    # Step 4\n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            param -= self.lr * param.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()\n",
    "                \n",
    "class LinearRegressionScratch(d2l.Module):  #@save\n",
    "    \"Our linear regression class, from https://d2l.ai/chapter_linear-regression/linear-regression-scratch.html, with SGD as the optimizer and with all the functions from the base Module class added.\"\n",
    "    def __init__(self, num_inputs, lr, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.board = d2l.ProgressBoard()\n",
    "        # Step 1 initialize weights and bias\n",
    "        self.w = torch.normal(0, sigma, (num_inputs, 1), requires_grad=True)\n",
    "        print(self.w)\n",
    "        self.b = torch.zeros(1, requires_grad=True)\n",
    "        print(self.b)\n",
    "\n",
    "    def plot(self, key, value, train):\n",
    "        \"\"\"Plot a point in animation.\"\"\"\n",
    "        assert hasattr(self, 'trainer'), 'Trainer is not inited'\n",
    "        self.board.xlabel = 'epoch'\n",
    "        if train:\n",
    "            x = self.trainer.train_batch_idx / \\\n",
    "                self.trainer.num_train_batches\n",
    "            n = self.trainer.num_train_batches / \\\n",
    "                self.plot_train_per_epoch\n",
    "        else:\n",
    "            x = self.trainer.epoch + 1\n",
    "            n = self.trainer.num_val_batches / \\\n",
    "                self.plot_valid_per_epoch\n",
    "        self.board.draw(x, value.to(d2l.cpu()).detach().numpy(),\n",
    "                        ('train_' if train else 'val_') + key,\n",
    "                        every_n=int(n))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return SGD([self.w, self.b], self.lr)\n",
    "        \n",
    "    def training_step(self, batch):\n",
    "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
    "        self.plot('loss', l, train=True)\n",
    "        return l\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        l = self.loss(self(*batch[:-1]), batch[-1])\n",
    "        self.plot('loss', l, train=False)\n",
    "\n",
    "    # Step 2 use the weights to make a prediction\n",
    "    def forward(self, X):\n",
    "        return torch.matmul(X, self.w) + self.b\n",
    "\n",
    "    # Step 3 calculate the loss\n",
    "    def loss(self, y_hat, y):\n",
    "        l = (y_hat - y) ** 2 / 2\n",
    "        return l.mean()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run it!\n",
    "\n",
    "We implement a reader for CSV data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class CsvData(d2l.DataModule):  #@save\n",
    "    def __init__(self, labelColIndex, path, batch_size=32):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        # read the data\n",
    "        df = pd.read_csv(path)\n",
    "        # drop any non-numeric columns\n",
    "        df = df._get_numeric_data()\n",
    "        # drop the label column from the features\n",
    "        colIndices = list(range(df.shape[1]))\n",
    "        colIndices.pop(labelColIndex)\n",
    "        features = df.iloc[:, colIndices]\n",
    "        # keep it in the label, obviously :)\n",
    "        labels = df.iloc[:, labelColIndex]\n",
    "        # split the dataset\n",
    "        self.train, self.val, self.train_y, self.val_y = train_test_split(features, labels, test_size=0.2, shuffle=True)\n",
    "\n",
    "    def get_dataloader(self, train):\n",
    "        features = self.train if train else self.val\n",
    "        labels = self.train_y if train else self.val_y\n",
    "        get_tensor = lambda x: torch.tensor(x.values, dtype=torch.float32)\n",
    "        tensors = (get_tensor(features), get_tensor(labels))\n",
    "        return self.get_tensorloader(tensors, train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get this data from https://archive-beta.ics.uci.edu/dataset/53/iris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegressionScratch(3, lr=1)\n",
    "data = CsvData(1,\"data/iris.data\", 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = d2l.Trainer(max_epochs=20)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What happens when you change the number of epochs?*\n",
    "\n",
    "*What happens when you change the learning rate?*\n",
    "\n",
    "*How does all this compare to your project 1 adaline class?*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Now write a short explanation of minibatch SGD suitable for a non-CS major. You can write a paragraph, make a diagram.... The method is up to you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
