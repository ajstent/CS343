{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "We use logistic regression when we want to *classify* things, i.e. when our label (dependent variable) is qualitative. Maybe we want to:\n",
    "* say whether each data point is \"in\" or \"out\" (*binary classification*) \n",
    "* assign each data point to a single class\n",
    "* assign each data point to one or more classes (*multi label classification*)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step one: reconfigure the labels\n",
    "\n",
    "Consider the iris data. The last column is the species; each iris is assigned to one of four species. \n",
    "\n",
    "**However**, a neural network only processes numbers. So we need our labels to become numeric somehow. We typically do this using a *one hot encoding*:\n",
    "1. First, we walk over the data and make a list $L$ of the unique labels. $||L|| = q$.\n",
    "2. Then, we walk over the data again, and replace each label with a vector of length $q$ in which every element is 0 except the one indexed at the index of this label in $L$, which will be 1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step two: reconfigure the network\n",
    "\n",
    "Instead of one node in the output layer, we have one for each label in the label list. \n",
    "\n",
    "We connect *all* input layer nodes to *each* output layer node.\n",
    "\n",
    "**Question**: The way this network is connected, it is described as what type of network?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step three: set the activation function\n",
    "\n",
    "Now that our labels are numeric, we could treat this as just a bunch of linear regressions (one per output node). However, if we do that we can't ensure that two important criteria for classification are met:\n",
    "* all the outputs are in [0, 1] \n",
    "* (for regular classification) their sum is 1\n",
    "\n",
    "Instead, we use the *softmax* function:\n",
    "$\\hat{y_i} = \\frac{exp(o_i)}{\\sum_j exp(o_j)}$ where $o_i$ is the output of the $i$ th output node.\n",
    "\n",
    "We often interpret the output of the network, a vector $\\hat{y}$, as containing the conditional probability of each class (label) given the features $x$. \n",
    "\n",
    "For more on the softmax: https://web.stanford.edu/~nanbhas/blog/sigmoid-softmax/."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step four: Set the loss function\n",
    "\n",
    "For a loss function, we use $-\\sum_{j=1}^q y_j \\log \\hat{y}_j$. This is called the *cross-entropy loss*.\n",
    "\n",
    "The entropy of a distribution $P$ is defined as:\n",
    " $\\sum_j -P(j) \\log P(j)$. \n",
    " \n",
    " This is:\n",
    "* the amount of space required to encode data drawn at random from $P$.\n",
    "* the amount of surprise one might expect at seeing an event $j$ drawn from $P$.\n",
    "\n",
    "You can think of this loss function as comparing the model's estimates of the probability of each class given the data, with the actual probability of each class given the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Take a Look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision\n",
    "!pip install d2l==1.0.0b0\n",
    "!pip install wandb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now import required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use the \"compact\" implementation of logistic regression as a neural network in pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(d2l.Classifier):  #@save\n",
    "    \"\"\"The logistic regression model.\"\"\"\n",
    "    def __init__(self, in_features, out_features, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(nn.Flatten(), nn.Linear(in_features, out_features))\n",
    "       \n",
    "    # why not computing softmax here? per the textbook: \"By combining softmax and cross-entropy, we can escape the numerical stability issues altogether.\"\n",
    "    def forward(self, X):\n",
    "        return self.net(X)\n",
    "\n",
    "    # cross-entropy loss\n",
    "    def loss(self, Y_hat, Y, averaged=True):\n",
    "        Y_hat = Y_hat.reshape((-1, Y_hat.shape[-1]))\n",
    "        Y = Y.reshape((-1,))\n",
    "        return F.cross_entropy(\n",
    "            Y_hat, Y, reduction='mean' if averaged else 'none')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use our reader for CSV data from last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class CsvData(d2l.DataModule):  #@save\n",
    "    def __init__(self, labelColIndex, path, batch_size=32, shuffle=True, split=0.2):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        # read the data\n",
    "        df = pd.read_csv(path)\n",
    "        self.labels = df.iloc[:, labelColIndex].unique()\n",
    "        colIndices = list(range(df.shape[1]))\n",
    "        colIndices.pop(labelColIndex)\n",
    "        features = df.iloc[:, colIndices]\n",
    "        # one hot encoding of the labels column\n",
    "        #labels = pd.get_dummies(df.iloc[:, labelColIndex])\n",
    "        labels = df.iloc[:, labelColIndex]\n",
    "        labels = labels.apply(lambda x: np.where(self.labels==x)[0][0]).astype(float)\n",
    "        # split the dataset\n",
    "        self.train, self.val, self.train_y, self.val_y = train_test_split(features, labels, test_size=split, shuffle=shuffle)\n",
    "        print(\"shuffle\", shuffle, \"batch_size\", batch_size, \"split\", split)\n",
    "        print(self.get_feature_count(), self.get_train_data_size(), self.get_test_data_size())\n",
    "         \n",
    "    def get_feature_count(self):\n",
    "        return self.train.shape[1]\n",
    "\n",
    "    def get_label_count(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_train_data_size(self):\n",
    "        return self.train.shape[0]\n",
    "\n",
    "    def get_test_data_size(self):\n",
    "        return self.val.shape[0]\n",
    "                \n",
    "    def text_labels(self, indices):\n",
    "        \"\"\"Return text labels.\"\"\"\n",
    "        return [self.labels[int(i)] for i in indices]\n",
    "\n",
    "    def get_dataloader(self, train):\n",
    "        features = self.train if train else self.val\n",
    "        labels = self.train_y if train else self.val_y\n",
    "        get_tensor = lambda x: torch.tensor(x.values, dtype=torch.float32)\n",
    "        tensors = (get_tensor(features), get_tensor(labels).long())\n",
    "        return self.get_tensorloader(tensors, train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set our hyperparameters, fit a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "epochs = 10\n",
    "split = 0.2\n",
    "batch_size = 12\n",
    "shuffle = True\n",
    "\n",
    "# You can get the iris data from https://archive-beta.ics.uci.edu/dataset/53/iris\n",
    "data = CsvData(4,\"data/iris.data\", batch_size=batch_size, shuffle=shuffle, split=split)\n",
    "model = LogisticRegression(data.get_feature_count(), data.get_label_count(), lr)\n",
    "\n",
    "# textbook has this: \n",
    "#data = d2l.FashionMNIST(batch_size=256)\n",
    "#model = LogisticRegression(784, 10, 0.1)\n",
    "\n",
    "trainer = d2l.Trainer(max_epochs=epochs)\n",
    "trainer.fit(model, data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
