{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR NAMES HERE**\n",
    "\n",
    "Spring 2023\n",
    "\n",
    "CS 343: Neural Networks\n",
    "\n",
    "Project 4: Recurrent Neural Networks\n",
    "\n",
    "**Submission reminders:**\n",
    "\n",
    "- Commit your code to git.\n",
    "- Did you answer all 10 questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for loading the datasets\n",
    "import preprocess_data\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Set the color style so that Professor Layton can see your plots\n",
    "plt.show()\n",
    "plt.style.use(['seaborn-colorblind', 'seaborn-darkgrid'])\n",
    "# Make the font size larger\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "# Turn off scientific notation when printing\n",
    "np.set_printoptions(suppress=True, precision=3)\n",
    "\n",
    "# Automatically reload external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def plot_cross_entropy_loss(loss_history):\n",
    "    plt.plot(loss_history)\n",
    "    plt.xlabel('Training iteration')\n",
    "    plt.ylabel('loss (cross-entropy)')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Implement Data Preprocessor\n",
    "\n",
    "## 1a. Implement the following functions in `preprocess_data.py`\n",
    "\n",
    "- `load_data`\n",
    "- `sample_sequence`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b. Load in simple data\n",
    "\n",
    "Load dollar_store_data.txt. This file contains a price list for a dollar store; each row contains a price for an item. All prices are between $1.00 and $9.99; all are of the format `\\$\\d\\\\.\\d\\d`.\n",
    "\n",
    "**Side note**: the set of these prices defines a *regular language*, which can be expressed as a *regular expression*. We are going to see how hard a RNN has to work to represent this language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_ix, ix_to_char, data = preprocess_data.load_data('data_regular.txt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b. Sample sequence\n",
    "\n",
    "Get a sample of length 6. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq = preprocess_data.sample_sequence(data, char_to_ix, 6, start=0)\n",
    "print(f'Your test sequence looks like {test_seq} and should look like {np.array([1, 10, 2, 3, 12, 0])}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1**: What is the vocabulary size for this dataset? Why?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 1**: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Train a MLP to predict the next character in a sequence\n",
    "\n",
    "## 2a. Copy over the MLP code from project 1\n",
    "\n",
    "Copy mlp.py.\n",
    "\n",
    "## 2b. Define a MLP\n",
    "\n",
    "In the cell below, define a MLP. Use an input width of size 5, one hidden layer of width len(char_to_ix)*2 with ReLU activation, and an output width of size len(char_to_ix) with softmax. Use cross entropy loss and minibatch SGD (just as you did in project 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlp import MLP\n",
    "\n",
    "net = MLP(5, len(char_to_ix)*2, len(char_to_ix))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2c. Sample data\n",
    "\n",
    "In the cell below, we sample 1000 fixed-length sequences for training data, 250 for dev and 250 for test. All sequences should be of length 6 (five features plus one class). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "dev = []\n",
    "test = []\n",
    "\n",
    "while(len(train) < 1000):\n",
    "    train.append(preprocess_data.sample_sequence(data, char_to_ix, 6, start=-1))\n",
    "train = np.array(train)\n",
    "train_x = train[:, 0:-1]\n",
    "train_y = train[:, -1]\n",
    "print(f'Shape of train: {train_x.shape, train_y.shape}')\n",
    "\n",
    "while(len(dev) < 250):\n",
    "    dev.append(preprocess_data.sample_sequence(data, char_to_ix, 6, start=-1))\n",
    "dev = np.array(dev)\n",
    "dev_x = dev[:, 0:-1]\n",
    "dev_y = dev[:, -1]\n",
    "print(f'Shape of dev: {dev_x.shape, dev_y.shape}')\n",
    "\n",
    "while(len(test) < 250):\n",
    "    test.append(preprocess_data.sample_sequence(data, char_to_ix, 6, start=-1))\n",
    "test = np.array(test)\n",
    "test_x = test[:, 0:-1]\n",
    "test_y = test[:, -1]\n",
    "print(f'Shape of test: {test_x.shape, test_y.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2d. Train and evaluate the MLP\n",
    "\n",
    "Train the MLP and evaluate using accuracy. Use these hyperparameters: `reg=0, print_every=10, lr=0.001, mini_batch_sz=50, n_epochs=500`.\n",
    "\n",
    "Plot the loss history.\n",
    "\n",
    "**NB**: This is kind of an artificial assessment: in the real world, a model would start with a seed sequence like \\$, and would then have to repeatedly predict the next character and add it to the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_hist, acc_train, acc_valid = net.fit(train_x, train_y, test_x, test_y, reg=0, print_every=10, lr=0.001, mini_batch_sz=50, n_epochs=500)\n",
    "\n",
    "plot_cross_entropy_loss(loss_hist)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**: Why are all the sequences of length six? What happens if you change the sequence length?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 2**:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Implement RNN with one hidden layer, tanh activation on the hidden layer and cross-entropy loss.\n",
    "\n",
    "The structure of our RNN will be:\n",
    "\n",
    "```\n",
    "Input layer (units to accommodate one one-hot encoded input at a time)) ->\n",
    "Hidden layer (Y units) with tanh activation ->\n",
    "Output layer (number of classes units) with softmax activation\n",
    "```\n",
    "\n",
    "You may be wondering why tanh activation. You can try reLu as an extension; it is subject to something called a \"dying ReLu\" problem. If you try reLu as an extension, implement leaky reLu. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3a. Implement the following functions in `rnn.py`\n",
    "\n",
    "- `initialize_wts`\n",
    "- `one_hot`\n",
    "- `predict`\n",
    "- `forward`\n",
    "- `backward`\n",
    "- `fit`\n",
    "\n",
    "For fit, use fixed truncation to unroll the RNN."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b. Test key functions with the dollar store dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rnn import RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = test_seq[0:5]\n",
    "test_y = test_seq[1:6]\n",
    "print(f'Vocab: {len(char_to_ix)}')\n",
    "print(f'Test input: {test_x}')\n",
    "print(f'Test output: {test_y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dummy net for testing\n",
    "num_inputs = len(char_to_ix)\n",
    "num_hidden_units = len(char_to_ix)*2\n",
    "num_layers = 1\n",
    "num_classes = len(char_to_ix)\n",
    "\n",
    "net = RNN(num_inputs, num_hidden_units, num_layers, num_classes, char_to_ix, ix_to_char)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3**: For this model, the number of nodes in the input layer and the number in the output layer should be the same. Why is this? Is it possible to have a RNN with more (or fewer) output layer nodes than input layer nodes?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 3**: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test `initialize_wts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.initialize_wts(std=0.01)\n",
    "print(f'xh wt shape, first hidden layer, is {net.xh_wts[0].shape} and should be (13, 26)')\n",
    "print(f'hh wt shape, first hidden layer, is {net.hh_wts[0].shape} and should be (26, 26)')\n",
    "print(f'h bias shape, first hidden layer, is {net.h_b[0].shape} and should be (26, 1)')\n",
    "print(f'hq wt shape is {net.hq_wts.shape} and should be (26, 13)')\n",
    "print(f'q bias shape is {net.q_b.shape} and should be (13, 1)')\n",
    "\n",
    "print(f'1st few xh wts are\\n{net.xh_wts[0][:,0]}\\nand should be\\n[ 0.018  0. -0.005 -0.003 -0.012 -0.008  0.011 -0.006  0.003 -0.012 -0.012 -0.01  -0.011]')\n",
    "print(f'1st few hh wts are\\n{net.hh_wts[0][:,0]}\\nand should be\\n[ -0.007 -0. 0.006  0.004 -0.001 -0.023  0.027 -0.002  0.011  0. -0.009 -0.004  0.003 -0.009 -0.008  0.008 -0. 0.015  0.005  0. -0.005 -0.008 -0.008 -0.01   0.003  0.014]')\n",
    "print(f'h bias is\\n{net.h_b[0].T}\\nand should be\\n[[-0.008 -0.009  0.002 -0.017  0.002  0.001  0.01   0.007 -0.004 -0.011 0.017 -0.008 -0.01  -0.011  0.011 -0.005 -0.008  0.001 -0.002 -0.007 0.008  0.011  0.01   0.008  0.004 -0.018]]')\n",
    "print(f'1st few hq wts are\\n{net.hq_wts[:,0]}\\nand should be\\n[ 0.017  0.01   0.001  0.003 -0.016 -0.003  0.013  0. 0.008  0.003 0.002 -0.004  0. 0.006  0.018 -0.016 -0.003 -0.014  0.004 -0.005 -0.014 -0.003  0.013  0.013  0.005 -0.006]')\n",
    "print(f'q bias is\\n{net.q_b.T}\\nand should be\\n[[-0.005  0.     0.01   0.002  0.009  0.015  0.004  0.012 -0.003 -0. -0.005  0.01   0.004]]')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the `predict` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = [np.zeros((net.num_hidden_units, 1)) for _ in range(net.num_layers)]\n",
    "test_y_pred = net.predict(h0, test_x[0], 5)\n",
    "print(f'Predicted classes are {np.array(test_y_pred)} and should be [9 12 11 7 1]')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the `forward` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = [np.zeros((net.num_hidden_units, 1)) for _ in range(net.num_layers)]\n",
    "\n",
    "xs, ps, hs, loss = net.forward(test_x, test_y, h0)\n",
    "\n",
    "print(f'Your loss is {loss} and should be 12.83962...')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the `backward` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw_xh, dw_hh, db_h, dw_hq, db_q = net.backward(test_x, test_y, xs, ps, hs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test fit\n",
    "\n",
    "\n",
    "Your `fit` function should show you print-outs showing:\n",
    "- Loss and sample predictions regularly during training.\n",
    "- After 5000 epochs of training, outputs that start to look like prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_hist = net.fit(train, num_steps=10, lr=0.001, n_epochs=5000)\n",
    "\n",
    "plot_cross_entropy_loss(loss_hist)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4**: Why do we not have held-out dev and test sets for training the RNN?\n",
    "\n",
    "**Question 5**: Why are we not evaluating using $R^2$? Why are we not evaluating using accuracy?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 4**: \n",
    "\n",
    "**Answer 5**: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2c. Train the RNN with the dollar store dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the cell below, define a RNN. Use an input width of size 5, one hidden layer of width len(char_to_ix)*2 with tanh activation, and an output width of size len(char_to_ix) with softmax. Use cross entropy loss and Adagrad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the RNN using the dollar store dataset. The RNN should have Use these hyperparameters: `print_every=100, lr=0.001, num_steps=10, n_epochs=500`.\n",
    "\n",
    "Plot the loss history and sample outputs as it trains. You should see a slow emergence of the occasional actual price-appearing token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6**: Which works better, the MLP or the RNN? Explain your answer.\n",
    "\n",
    "**Question 7**: Apart from the network architecture, what other differences are there between the MLP and the RNN? (Think: activation functions, optimization algorithms...)\n",
    "\n",
    "**Question 8**: Add a second hidden layer to the RNN. Does this change the performance of the model?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 6**:\n",
    "\n",
    "**Answer 7**:\n",
    "\n",
    "**Answer 8**:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Train RNN on the arithmetic dataset\n",
    "\n",
    "## 2a. Load the arithmetic dataset\n",
    "\n",
    "Load data_calculator.txt. This file contains inputs to a regular infix calculator. \n",
    "\n",
    "**Side note**: the set of these inputs defines a *context-free language*, which can be expressed as a *context-free grammar*. We are going to see how hard a RNN has to work to represent this language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b. Implement and test regular and randomized truncation\n",
    "\n",
    "So far, we've been truncating backprop at a fixed number of time steps. Extend the fit and backward functions of the RNN class to take a named argument for type of truncation (none, regular or randomized) and a named argument for sequence length (for regular truncation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2c. Compare regular truncation and randomized truncation\n",
    "\n",
    "In the cells below, fit a RNN to the arithmetic dataset using each approach to backpropagation through time. Otherwise, hold the hyperparameters constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 9**: Which works best for this dataset, regular truncation or randomized truncation? Why?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 9**:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5: Train RNN on recipe dataset\n",
    "\n",
    "## 5a. Load the recipe dataset\n",
    "\n",
    "Load data_recipes.txt. This data comes from https://recipenlg.cs.put.poznan.pl/ and is made available for non-commercial research/teaching use *only*.\n",
    "\n",
    "**Side note**: the set of these inputs may or may not define a context-free language. For sure it's got a bigger vocabulary than our previous datasets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b. Train a RNN on this dataset\n",
    "\n",
    "In cells below:\n",
    "- Train a RNN using the dollar store data. Configure the RNN with the following non-default hyperparameters:\n",
    "    - 200 hidden units\n",
    "    - 1 hidden layer\n",
    "    - Learning rate of 0.0001\n",
    "    - Sequence length of 40\n",
    "    - 30000 epochs\n",
    "- Plot the loss over training iterations. You should see a slow emergence of the occasional actual recipe direction ish phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 10**: How many epochs of training does it take before you start to get recipe-type text out? Why does this dataset take longer?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 10**:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extensions\n",
    "\n",
    "**Reminder**: Please do not integrate extensions into your base project so that it changes the expected behavior of core functions. It is better to duplicate the base project and add features from there.\n",
    "\n",
    "1) Add more hidden layers to the RNN.\n",
    "\n",
    "2) Implement dropout in the RNN.\n",
    "\n",
    "3) Extend the RNN into a LSTM or GRU.\n",
    "\n",
    "4) Implement visualization of the RNN (as in Karpathy's code)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
