{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision\n",
    "!pip install d2l==1.0.0b0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "Definition: \"a statistical method ... that attempts to determine the strength and character of the relationship between one dependent variable (usually denoted by Y) and a series of other variables (known as independent variables).\" (Investopedia)\n",
    "\n",
    "Types:\n",
    "* **linear**\n",
    "* *logistic*\n",
    "* polynomial\n",
    "* ridge\n",
    "* lasso\n",
    "* elastic net\n",
    "* poisson\n",
    "* quantile\n",
    "\n",
    "(and probably more!)\n",
    "\n",
    "How to fit:\n",
    "* analytical solutions\n",
    "* approximation solutions\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "$y = w_1 x_1 + w_2 x_2 + ... + b$\n",
    "\n",
    "The $w$ s are *weights*.\n",
    "\n",
    "The $b$ is the *bias* (offset, intercept).\n",
    "\n",
    "The $x$ s are  values of independent values (features).\n",
    "\n",
    "The $y$ is the value of the dependent variable (label). For linear regression, the $y$ should be quantitative.\n",
    "\n",
    "This function fits a **line**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression and Matrix Math\n",
    "\n",
    "Let's say we have ten data points and three independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[0,2,1], \n",
    "        [0,1,1], \n",
    "        [0,2,0], \n",
    "        [0,1,0], \n",
    "        [1,2,0], \n",
    "        [2,0,1], \n",
    "        [2,1,1], \n",
    "        [1,0,0], \n",
    "        [1,1,1]]\n",
    "\n",
    "y = [6,4,5,3,6,4,6,2,5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to figure out the $w$s and the $b$. Let's start by just making a guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [1,1,1]\n",
    "bias = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use the dot product to figure out $\\hat{y}$, the guesses at $y$ we get from our $w$s and $b$.\n",
    "\n",
    "Now the shape of x is (3,1) and the shape of w is (3,1), so if we want to take a dot product we have to first take the transpose of w.\n",
    "\n",
    "*Can you write this with just numpy?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "w = torch.tensor(weights)\n",
    "\n",
    "yHat = []\n",
    "for row in data:\n",
    "    x = torch.tensor(row)\n",
    "    yHat.append((w.T.dot(x) + bias).tolist())\n",
    "\n",
    "print(yHat)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, we can just do it all at once for all the data points!\n",
    "\n",
    "*Can you write this with just numpy?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([x[0:3] for x in data])\n",
    "print(X)\n",
    "\n",
    "yHat = X@w + bias\n",
    "print(yHat.tolist())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How good was our guess?\n",
    "\n",
    "One measure we often use for \"goodness of fit\" of a linear regression is *mean sum of squared error*: $\\frac{\\sum_i \\frac{1}{2}(\\hat{y_i}-y_i)^2}{||y||}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse = 0\n",
    "for i in range(len(y)):\n",
    "    sse += 1/2*(yHat[i]-y[i])**2\n",
    "\n",
    "print(sse/len(y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we had picked our $w$ s and $b$ perfectly, what would the mean sum of squared error be?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression as a neural network\n",
    "\n",
    "Right, so I think we can now frame linear regression as a neural network.\n",
    "\n",
    "* The type of machine learning task will be regression.\n",
    "* The *model architecture* will be a neural network.\n",
    "  * The *width* will be the number of independent variables we have, plus one for the $b$.\n",
    "  * The *depth* will be 1 - we have our input layer, then our output layer, and we are done.\n",
    "  * The *connectedness* will be feedforward. We send data from the input layer to the output layer and that's all - no nodes in the input layer talk to each other.\n",
    "  * The parameters of the model will be the $w$s and the $b$. These will be on the edges connecting the input layer to the output neuron.\n",
    "  * Sometimes there are *hyperparameters* of the model architecture, but here there aren't any.\n",
    "* The *loss function* (or optimization function) will be sum of squared error.\n",
    "* We will fit this architecture using *gradient descent*.\n",
    "  * This always has a *hyperparameter* for learning rate.\n",
    "  * There may be other hyperparameters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In pytorch\n",
    "\n",
    "First, we install required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make the linear regression model architecture. Notice how in the constructor we have to specify:\n",
    "* the number of inputs (width of the input layer)\n",
    "* the learning rate (a *hyperparameter* of the optimization algorithm)\n",
    "* sigma (another *hyperparameter* of the otpimization algorithm)\n",
    "\n",
    "Then, we define how the neural network's neurons are connected.\n",
    "* When we run a neural network - when we push data through til the output layer on a single data point - that's called a *forward pass*. \n",
    "* The forward pass implementation tells us how the input layer is connected to the output layer. We always represent this as matrix multiply of some kind.\n",
    "* *What does the forward pass do in this case?*\n",
    "\n",
    "And then, we define the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionScratch(d2l.Module):  #@save\n",
    "    def __init__(self, num_inputs, lr, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.w = torch.normal(0, sigma, (num_inputs, 1), requires_grad=True)\n",
    "        print(self.w)\n",
    "        self.b = torch.zeros(1, requires_grad=True)\n",
    "        print(self.b)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return torch.matmul(X, self.w) + self.b\n",
    "    \n",
    "    def loss(self, y_hat, y):\n",
    "        l = (y_hat - y) ** 2 / 2\n",
    "        return l.mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we said before we would use *gradient descent* to train this model. Remember the boats from last class session?\n",
    "\n",
    "Classic (pure) gradient descent goes like this:\n",
    "1. Run *forward* on all your data.\n",
    "2. Calculate the derivative of the *loss*.\n",
    "3. Multiply the gradient by the *learning rate*.\n",
    "4. Subtract that value from each of the model's parameters (in this case, $w$s and $b$). This step is called *backpropagation*.\n",
    "\n",
    "However, this can be computationally expensive. There are many variants of gradient descent that are more efficient, including:\n",
    "* Stochastic gradient descent - at each round, pick one training data point (at random) and run gradient descent just with that. \n",
    "* Minibatch stochastic gradient descent - at each round, pick $n$ training data points (at random) and run gradient descent just with that. (Normalize the learning rate by the minibatch size.)\n",
    "\n",
    "What do we initialize the $w$ s and $b$ to at the beginning? Random values.\n",
    "\n",
    "When do we stop? When either of the following is satisfied:\n",
    "1. The loss has pretty much stopped changing.\n",
    "2. We've run it for a preset maximum number of *epochs*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(d2l.HyperParameters):  #@save\n",
    "    def __init__(self, params, lr):\n",
    "        \"\"\"Minibatch stochastic gradient descent.\"\"\"\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            param -= self.lr * param.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add this optimization algorithm to our linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(LinearRegressionScratch)  #@save\n",
    "def configure_optimizers(self):\n",
    "    return SGD([self.w, self.b], self.lr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we augment the default pytorch trainer (defined in the book). It implements:\n",
    "* for each epoch:\n",
    "  * for each minibatch in the training data:\n",
    "    * run forward (training_step)\n",
    "    * calculate the loss\n",
    "    * backward propagate the loss \n",
    "  * run evaluation on the validation data, report loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(d2l.Trainer)  #@save\n",
    "def prepare_batch(self, batch):\n",
    "    return batch\n",
    "\n",
    "@d2l.add_to_class(d2l.Trainer)  #@save\n",
    "def fit_epoch(self):\n",
    "    self.model.train()\n",
    "    for batch in self.train_dataloader:\n",
    "        loss = self.model.training_step(self.prepare_batch(batch))\n",
    "        self.optim.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            loss.backward()\n",
    "            if self.gradient_clip_val > 0:  # To be discussed later\n",
    "                self.clip_gradients(self.gradient_clip_val, self.model)\n",
    "            self.optim.step()\n",
    "        self.train_batch_idx += 1\n",
    "    if self.val_dataloader is None:\n",
    "        return\n",
    "    self.model.eval()\n",
    "    for batch in self.val_dataloader:\n",
    "        with torch.no_grad():\n",
    "            self.model.validation_step(self.prepare_batch(batch))\n",
    "        self.val_batch_idx += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegressionScratch(2, lr=0.03)\n",
    "data = d2l.SyntheticRegressionData(w=torch.tensor([2, -3.4]), b=4.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = d2l.Trainer(max_epochs=3)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What happens when you change the number of epochs?*\n",
    "\n",
    "*What happens when you change the learning rate?*\n",
    "\n",
    "*How does all this compare to your project 1 adaline class?*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
