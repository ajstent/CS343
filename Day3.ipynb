{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision\n",
    "!pip install d2l==1.0.0b0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That TensorBoard playground\n",
    "\n",
    "Did you beat me?\n",
    "\n",
    "![image.png](attachment:image-2.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "Definition: \"a statistical method ... that attempts to determine the strength and character of the relationship between one dependent variable (usually denoted by Y) and a series of other variables (known as independent variables).\" (Investopedia)\n",
    "\n",
    "Types:\n",
    "* **linear**\n",
    "* *logistic*\n",
    "* polynomial\n",
    "* ridge\n",
    "* lasso\n",
    "* elastic net\n",
    "* poisson\n",
    "* quantile\n",
    "\n",
    "(and probably more!)\n",
    "\n",
    "How to fit:\n",
    "* analytical solutions\n",
    "* approximation solutions\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "$y = w_1 x_1 + w_2 x_2 + ... + b$\n",
    "\n",
    "The $w$ s are *weights*.\n",
    "\n",
    "The $b$ is the *bias* (offset, intercept).\n",
    "\n",
    "The $x$ s are  values of independent values (features).\n",
    "\n",
    "The $y$ is the value of the dependent variable (label). For linear regression, the $y$ should be quantitative.\n",
    "\n",
    "This function fits a **line**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression and Matrix Math\n",
    "\n",
    "Let's say we have ten data points and three independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[0,2,1], \n",
    "        [0,1,1], \n",
    "        [0,2,0], \n",
    "        [0,1,0], \n",
    "        [1,2,0], \n",
    "        [2,0,1], \n",
    "        [2,1,1], \n",
    "        [1,0,0], \n",
    "        [1,1,1]]\n",
    "\n",
    "y = [6,4,5,3,6,4,6,2,5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to figure out the $w$s and the $b$. Let's start by just making a guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [1,1,1]\n",
    "bias = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use the dot product to figure out $\\hat{y}$, the guesses at $y$ we get from our $w$s and $b$.\n",
    "\n",
    "Now the shape of x is (3,1) and the shape of w is (3,1), so if we want to take a dot product we have to first take the transpose of w.\n",
    "\n",
    "*Can you write this with just numpy?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "w = torch.tensor(weights)\n",
    "\n",
    "yHat = []\n",
    "for row in data:\n",
    "    x = torch.tensor(row)\n",
    "    yHat.append((w.T.dot(x) + bias).tolist())\n",
    "\n",
    "print(yHat)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, we can just do it all at once for all the data points!\n",
    "\n",
    "*Can you write this with just numpy?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([x[0:3] for x in data])\n",
    "print(X)\n",
    "\n",
    "yHat = X@w + bias\n",
    "print(yHat.tolist())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How good was our guess?\n",
    "\n",
    "One measure we often use for \"goodness of fit\" of a linear regression is *mean sum of squared error*: $\\frac{\\sum_i \\frac{1}{2}(\\hat{y_i}-y_i)^2}{||y||}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse = 0\n",
    "for i in range(len(y)):\n",
    "    sse += 1/2*(yHat[i]-y[i])**2\n",
    "\n",
    "print(sse/len(y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we had picked our $w$ s and $b$ perfectly, what would the mean sum of squared error be?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression as a neural network\n",
    "\n",
    "Right, so I think we can now frame linear regression as a neural network.\n",
    "\n",
    "* The type of machine learning task will be regression.\n",
    "* The *model architecture* will be a neural network.\n",
    "  * The *width* will be the number of independent variables we have, plus one for the $b$.\n",
    "  * The *depth* will be 1 - we have our input layer, then our output layer, and we are done.\n",
    "  * The *connectedness* will be feedforward. We send data from the input layer to the output layer and that's all - no nodes in the input layer talk to each other.\n",
    "  * The parameters of the model will be the $w$s and the $b$. These will be on the edges connecting the input layer to the output neuron.\n",
    "  * Sometimes there are *hyperparameters* of the model architecture, but here there aren't any.\n",
    "* The *loss function* (or optimization function) will be sum of squared error.\n",
    "* We will fit this architecture using *gradient descent*.\n",
    "  * This always has a *hyperparameter* for learning rate.\n",
    "  * There may be other hyperparameters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In pytorch\n",
    "\n",
    "First, we install required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make the linear regression model architecture. Notice how in the constructor we have to specify:\n",
    "* the number of inputs (width of the input layer)\n",
    "* the learning rate (a *hyperparameter* of the optimization algorithm)\n",
    "* sigma (another *hyperparameter* of the otpimization algorithm)\n",
    "\n",
    "Then, we define how the neural network's neurons are connected.\n",
    "* When we run a neural network - when we push data through til the output layer on a single data point - that's called a *forward pass*. \n",
    "* The forward pass implementation tells us how the input layer is connected to the output layer. We always represent this as matrix multiply of some kind.\n",
    "* *What does the forward pass do in this case?*\n",
    "\n",
    "And then, we define the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionScratch(d2l.Module):  #@save\n",
    "    def __init__(self, num_inputs, lr, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.w = torch.normal(0, sigma, (num_inputs, 1), requires_grad=True)\n",
    "        print(self.w)\n",
    "        self.b = torch.zeros(1, requires_grad=True)\n",
    "        print(self.b)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return torch.matmul(X, self.w) + self.b\n",
    "    \n",
    "    def loss(self, y_hat, y):\n",
    "        l = (y_hat - y) ** 2 / 2\n",
    "        return l.mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we said before we would use *gradient descent* to train this model. Remember the boats from last class session?\n",
    "\n",
    "Classic (pure) gradient descent goes like this:\n",
    "1. Run *forward* on all your data.\n",
    "2. Calculate the derivative of the *loss*.\n",
    "3. Multiply the gradient by the *learning rate*.\n",
    "4. Subtract that value from each of the model's parameters (in this case, $w$s and $b$). This step is called *backpropagation*.\n",
    "\n",
    "However, this can be computationally expensive. There are many variants of gradient descent that are more efficient, including:\n",
    "* Stochastic gradient descent - at each round, pick one training data point (at random) and run gradient descent just with that. \n",
    "* Minibatch stochastic gradient descent - at each round, pick $n$ training data points (at random) and run gradient descent just with that. (Normalize the learning rate by the minibatch size.)\n",
    "\n",
    "What do we initialize the $w$ s and $b$ to at the beginning? Random values.\n",
    "\n",
    "When do we stop? When either of the following is satisfied:\n",
    "1. The loss has pretty much stopped changing.\n",
    "2. We've run it for a preset maximum number of *epochs*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(d2l.HyperParameters):  #@save\n",
    "    def __init__(self, params, lr):\n",
    "        \"\"\"Minibatch stochastic gradient descent.\"\"\"\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def step(self):\n",
    "        for param in self.params:\n",
    "            param -= self.lr * param.grad\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for param in self.params:\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add this optimization algorithm to our linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(LinearRegressionScratch)  #@save\n",
    "def configure_optimizers(self):\n",
    "    return SGD([self.w, self.b], self.lr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we augment the default pytorch trainer (defined in the book). It implements:\n",
    "* for each epoch:\n",
    "  * for each minibatch in the training data:\n",
    "    * run forward (training_step)\n",
    "    * calculate the loss\n",
    "    * backward propagate the loss \n",
    "  * run evaluation on the validation data, report loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(d2l.Trainer)  #@save\n",
    "def prepare_batch(self, batch):\n",
    "    return batch\n",
    "\n",
    "@d2l.add_to_class(d2l.Trainer)  #@save\n",
    "def fit_epoch(self):\n",
    "    self.model.train()\n",
    "    for batch in self.train_dataloader:\n",
    "        loss = self.model.training_step(self.prepare_batch(batch))\n",
    "        self.optim.zero_grad()\n",
    "        with torch.no_grad():\n",
    "            loss.backward()\n",
    "            if self.gradient_clip_val > 0:  # To be discussed later\n",
    "                self.clip_gradients(self.gradient_clip_val, self.model)\n",
    "            self.optim.step()\n",
    "        self.train_batch_idx += 1\n",
    "    if self.val_dataloader is None:\n",
    "        return\n",
    "    self.model.eval()\n",
    "    for batch in self.val_dataloader:\n",
    "        with torch.no_grad():\n",
    "            self.model.validation_step(self.prepare_batch(batch))\n",
    "        self.val_batch_idx += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegressionScratch(2, lr=0.03)\n",
    "data = d2l.SyntheticRegressionData(w=torch.tensor([2, -3.4]), b=4.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = d2l.Trainer(max_epochs=3)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What happens when you change the number of epochs?*\n",
    "\n",
    "*What happens when you change the learning rate?*\n",
    "\n",
    "*How does all this compare to your project 1 adaline class?*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
