{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR NAMES HERE**\n",
    "\n",
    "Fall 2022\n",
    "\n",
    "CS 343: Neural Networks\n",
    "\n",
    "Project 2: Multi-layer Perceptrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for obtaining the STL-dataset\n",
    "import load_stl10_dataset\n",
    "\n",
    "# for preprocessing dataset\n",
    "import preprocess_data\n",
    "\n",
    "# Set the color style so that Professor Layton can see your plots\n",
    "plt.show()\n",
    "plt.style.use(['seaborn-colorblind', 'seaborn-darkgrid'])\n",
    "# Make the font size larger\n",
    "plt.rcParams.update({'font.size': 20})\n",
    "\n",
    "# Turn off scientific notation when printing\n",
    "np.set_printoptions(suppress=True, precision=3)\n",
    "\n",
    "# Automatically reload external modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Implement single layer network to test softmax activation and cross-entropy loss\n",
    "\n",
    "You will first implement and test out the softmax activation and cross-entropy loss in a single layer net before embedding it in a more complex multi-layer network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Load in preprocessed STL-10 data\n",
    "\n",
    "Use your automated preprocessing function to load in the STL-10 data in the following split:\n",
    "- 3000 training samples\n",
    "- 750 test samples\n",
    "- 1000 validation samples\n",
    "- 250 samples for development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Implement the following functions\n",
    "\n",
    "In `softmax_layer.py`, implement the following methods in the base class `SoftmaxLayer`:\n",
    "\n",
    "- `fit`\n",
    "- `net_in`\n",
    "- `predict`\n",
    "- `one_hot`\n",
    "- `accuracy`\n",
    "- `activation` (softmax) $f(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^C e^{x_j}}$ where $x_i$ are the \"net in\" values and there are $C$ output neurons (one per input class). $f(x_i)$ is the activation values of each output neuron $i$. Since this is softmax, it is the probability that a given input belongs to the class $i$ coded by the output neuron.\n",
    "- `loss` (cross-entropy) $L(x_m) = -\\frac{1}{B}\\sum_{b=1}^B{Log \\left (\\frac{e^{x_m}}{\\sum_{n=1}^C e^{x_n}}\\right )}$. $m$ is the correct class for the $b^{th}$ input. $x_m$ is the output neuron activation for the correct class, $x_n$ is the output neuron activation for all of the classes (in the sum). The batch size is $B$, so the loss is averaged over each mini-batch of inputs. The expression in the $Log$ is just the softmax.\n",
    "- `gradient` (for softmax/cross-entropy)\n",
    "\n",
    "You're welcome to work in any order, but I recommend starting with `fit` because as you work though it, you should recognize why we need most of the other methods. You can finish `fit` or branch off as you need the other methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c. Test key functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from softmax_layer import SoftmaxLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate some small Gaussian weights equal to the length of an image feature vector\n",
    "np.random.seed(0)\n",
    "randWts = np.random.normal(loc=0, scale=0.01, size=(x_dev.shape[1], 10))\n",
    "b = 1\n",
    "softmaxNet = SoftmaxLayer(-1)\n",
    "\n",
    "# Fake data for consistent debugging\n",
    "test_imgs = np.random.random(size=(15, x_dev.shape[1])) - 0.5\n",
    "test_labels = np.random.randint(low=0, high=6, size=(15,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the `onehot` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test1 = np.array([2, 2, 0, 1])\n",
    "c_test = 4\n",
    "y_one_hot = softmaxNet.one_hot(y_test1, c_test)\n",
    "print(f'Your one hot vectors:\\n{y_one_hot}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your one hot vectors should look like:\n",
    "\n",
    "    [[0. 0. 1. 0.]\n",
    "     [0. 0. 1. 0.]\n",
    "     [1. 0. 0. 0.]\n",
    "     [0. 1. 0. 0.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the `loss`,  `net_in`, softmax `activation` functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lossNoReg, lossReg = softmaxNet.test_loss(randWts, b, test_imgs, test_labels)\n",
    "print(f'The loss (without regularization) is {lossNoReg:.2f} and it should approx be 2.37')\n",
    "print(f'The loss (with 0.5 regularization) is {lossReg:.2f} and it should approx be 3.13')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the `gradient` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_wts, grad_b = softmaxNet.test_gradient(randWts, b, test_imgs, test_labels, 10)\n",
    "print()\n",
    "print(f'1st few Wt gradient values are {grad_wts[:4,0]}\\nand should be                  [-0.012  0.003  0.025 -0.023] ')\n",
    "print(f'1st few Wt bias values are {grad_b[:4]}\\nand should be              [-0.101 -0.099 -0.037  0.101]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test `fit` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "softmaxNet = SoftmaxLayer(10)\n",
    "loss_history = softmaxNet.fit(x_dev, y_dev, n_epochs=600, mini_batch_sz=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the random mini-batch sampling process, you may get different specific numbers, but the loss should generally decrease over iterations. You should get something like this:\n",
    "\n",
    "```\n",
    "Starting to train network...There will be 600 epochs and 600 iterations total, 1 iter/epoch.\n",
    "  Completed iter 0/600. Training loss: 2.46.\n",
    "  Completed iter 100/600. Training loss: 2.16.\n",
    "  Completed iter 200/600. Training loss: 2.02.\n",
    "  Completed iter 300/600. Training loss: 1.91.\n",
    "  Completed iter 400/600. Training loss: 1.92.\n",
    "  Completed iter 500/600. Training loss: 1.80.\n",
    "Finished training!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot the loss\n",
    "\n",
    "It should look noisy, but decrease on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cross_entropy_loss(loss_history):\n",
    "    plt.plot(loss_history)\n",
    "    plt.xlabel('Training iteration')\n",
    "    plt.ylabel('loss (cross-entropy)')\n",
    "    plt.show()\n",
    "    \n",
    "plot_cross_entropy_loss(loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2d. Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.** What do you think the decrease in loss over the epochs tells us about the state of the training process? How is current training going? What's the future potential like?\n",
    "\n",
    "**Question 2.** Below, write code to test regularization with training: check to make sure that the loss starts at ~154 for `reg=100`. Once you get this working, play around with the regularization parameter. You can drastically change the magnitude, but it should always remain nonnegative. How does regularization affect the training loss and **why**?\n",
    "\n",
    "**Question 3.** Play around with the batch size parameter. How does this affect the training loss and **why**? (*Think about the error gradient and how the weights change*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "# Your code here\n",
    "softmaxNet = SoftmaxLayer(10)\n",
    "loss_history = softmaxNet.fit(x_dev, y_dev, reg=100, n_epochs=600, mini_batch_sz=250)\n",
    "plot_cross_entropy_loss(loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 1:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 2:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 3:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2e. Train and optimize STL-10 dataset performance\n",
    "\n",
    "As you've surely noticed, hyperparameters can drastically affect learning! \n",
    "\n",
    "\n",
    "Implement a grid search for the best **combinations** of hyperparameters\n",
    "\n",
    "- learning rate,\n",
    "- regularization\n",
    "- batch size \n",
    "\n",
    "The grid search process should:\n",
    "\n",
    "1. Fit the model with specific values of hyperparameters that we're testing (using the training set).\n",
    "2. Compute the accuracy on the **training set**. \n",
    "3. Compute the accuracy on the **validation set.** \n",
    "4. Print out and record the best parameter combination as you go (that *improves* the **validation set accuracy**).\n",
    "5. Wipe the weights clean (reinitialize them) every time you try new parameters. It's easiest just to create a new net object on each run.\n",
    "\n",
    "#### Suggestions\n",
    "\n",
    "This can take quite a bit of simulation time! Here are some tips:\n",
    "- I suggest using a coarse-to-fine search strategy. First try varying parameters over many orders of magnitude. Use the \"new best\" print outs to refine the ranges that you test. Abort simulations prematurely if you feel there aren't productive (no reason to wait!). This can take however long or short that you want to dedicate. Remember, you are printing out the best parameter values on each run, so you can just proceed with those.\n",
    "- You should be able to achieve ~30% accuracy without too much effort (10% is chance performance).\n",
    "- High learning rates don't really make sense. You'll know if your value is \"high\" if numpy complains about numerical issues.\n",
    "- Your mini-batch sizes should be <= N and >= 1.\n",
    "- Time single network runs with a few different batch sizes you plan on trying in your big search. This will help you figure out a ballpark estimate how long grid search will take (*you can decide whether to go eat dinner, run it overnight, etc.*). If it will take an unreasonable amount of time, reduce the number of parameters you try in one search.\n",
    "- Think about whether you need *3 nested loops* or *a sequence of single loops*.\n",
    "- Pick a value for the number of training epochs that seems reasonable to you and stick with it for the grid search.\n",
    "- Turn off print outs from `fit` (adjust `verbose` argument) and only print out things related to your search.\n",
    "\n",
    "**Important note:** I am not grading based on your performance numbers or the number of hours your computer spends searching. I want to see that you successfully implemented the grid search to find progressively better hyperparameters on STL-10 and use the outcome to inform your ultimate training session that you use to evaluate predictions on the test set. *Getting full credit here does not require you spending hours of searching!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2f. Evaluate best model on test set\n",
    "\n",
    "**Question 4:** Now that you have \"good\" parameter values recorded, train a new model with the best learning rate, regularization strength, and batch size values in the cell below. What accuracy do you get on the **test set**? How does this compare to chance performance?\n",
    "\n",
    "*Recall: The test set should NOT be used in your grid search. It should only be processed once AFTER you conclude your grid search.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 4:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2g. Visualize learned weights\n",
    "\n",
    "Run the following code that plots the network weights going to each output neuron. If all goes well, you should see something really cool! Include the plot in your submitted project to show me what you got!\n",
    "\n",
    "**Note:** the quality of your visualizations will depend on:\n",
    "- The quality of the hyperparameters that you got via grid search.\n",
    "- How many epochs that you trained the network before plotting the weights\n",
    "\n",
    "One extension idea: is to find the combination of the above that result in the best visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the class names\n",
    "classes = np.loadtxt(os.path.join('data', 'stl10_binary', 'class_names.txt'), dtype=str)\n",
    "\n",
    "# We don't care about the bias wt\n",
    "wts = bestNet.wts\n",
    "# Reshape the wt vectors into spatial 'image' configurations to visualization\n",
    "wts = wts.reshape(32, 32, 3, 10)\n",
    "\n",
    "# Make a large new empty figure/plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "# Loop through each output neuron\n",
    "for i in range(10):\n",
    "  # Make a 2x5 grid of images\n",
    "  plt.subplot(2, 5, i+1)\n",
    "  \n",
    "  # Rescale the weights to be between 0 and 255\n",
    "  currImg = 255.0 * (wts[:, :, :, i].squeeze() - np.min(wts)) / (np.max(wts) - np.min(wts))\n",
    "  \n",
    "  plt.imshow(currImg.astype('uint8'))\n",
    "  plt.axis('off')\n",
    "  plt.title(classes[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
